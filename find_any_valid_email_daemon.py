#!/usr/bin/env python3
"""
Daemon TURBO pour rechercher des emails valides en continu
ExÃ©cute find_any_valid_email.py en boucle avec traitement parallÃ¨le
"""

import asyncio
import signal
import sys
import time
from datetime import datetime
from database import get_session, Site
import logging

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/var/www/Scrap_Email/find_any_valid_email_daemon.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Flag pour arrÃªter proprement le daemon
running = True

def signal_handler(sig, frame):
    """Gestion de l'arrÃªt propre du daemon"""
    global running
    logger.info("ğŸ›‘ ArrÃªt du daemon demandÃ©...")
    running = False

signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)


def count_sites_to_process():
    """Compter les sites sans email Ã  traiter"""
    session = get_session()
    try:
        count = session.query(Site).filter(
            Site.is_active == True,
            Site.blacklisted == False,
            (
                (Site.emails == "NO EMAIL FOUND") |
                (Site.emails == None) |
                (Site.emails == "")
            ),
            # Exclure les sites dÃ©jÃ  testÃ©s avec guessing
            ~Site.email_source.like('%any_valid%')
        ).count()
        return count
    finally:
        session.close()


async def run_email_finder(batch_size: int = 100, limit_per_run: int = 500,
                           max_concurrent: int = 50, parallel_sites: int = 20):
    """
    ExÃ©cuter le finder d'emails - VERSION TURBO

    Args:
        batch_size: Taille des lots
        limit_per_run: Nombre max de sites par exÃ©cution
        max_concurrent: Nombre max de requÃªtes HTTP simultanÃ©es
        parallel_sites: Nombre de sites traitÃ©s en parallÃ¨le
    """
    from find_any_valid_email import AnyValidEmailFinder

    finder = AnyValidEmailFinder(max_concurrent=max_concurrent)

    await finder.process_all(
        limit=limit_per_run,
        batch_size=batch_size,
        parallel_sites=parallel_sites
    )


async def daemon_loop(check_interval: int = 60, batch_size: int = 100,
                      limit_per_run: int = 2000, max_concurrent: int = 100,
                      parallel_sites: int = 20):
    """
    Boucle principale du daemon - VERSION TURBO

    Args:
        check_interval: Intervalle entre les vÃ©rifications en secondes
        batch_size: Taille des lots
        limit_per_run: Nombre max de sites par exÃ©cution
        max_concurrent: Nombre max de requÃªtes HTTP simultanÃ©es
        parallel_sites: Nombre de sites traitÃ©s en parallÃ¨le
    """
    global running

    logger.info("=" * 70)
    logger.info("ğŸš€ DÃ‰MARRAGE DU DAEMON - Find Any Valid Email - VERSION TURBO")
    logger.info("=" * 70)
    logger.info(f"   Intervalle de vÃ©rification: {check_interval}s ({check_interval/60:.1f} min)")
    logger.info(f"   Batch size: {batch_size}")
    logger.info(f"   Limite par exÃ©cution: {limit_per_run}")
    logger.info(f"   Concurrence HTTP: {max_concurrent}")
    logger.info(f"   Sites en parallÃ¨le: {parallel_sites}")
    logger.info("=" * 70)

    total_processed = 0
    start_time = datetime.now()

    while running:
        try:
            # Compter les sites Ã  traiter
            sites_to_process = count_sites_to_process()

            if sites_to_process > 0:
                logger.info(f"\nğŸ“Š {sites_to_process} sites sans email Ã  traiter")
                logger.info("ğŸš€ Lancement du traitement TURBO...")

                run_start = time.time()
                await run_email_finder(
                    batch_size=batch_size,
                    limit_per_run=limit_per_run,
                    max_concurrent=max_concurrent,
                    parallel_sites=parallel_sites
                )
                run_duration = time.time() - run_start

                processed_this_run = min(sites_to_process, limit_per_run)
                total_processed += processed_this_run
                speed = processed_this_run / run_duration if run_duration > 0 else 0

                logger.info(f"âœ… Traitement terminÃ© en {run_duration:.1f}s ({speed:.1f} sites/s)")
                logger.info(f"ğŸ“ˆ Total traitÃ©s depuis le dÃ©marrage: {total_processed}")

            else:
                logger.info(f"âœ… Aucun nouveau site Ã  traiter")

            # Attendre avant la prochaine vÃ©rification
            if running:
                logger.info(f"ğŸ’¤ Pause de {check_interval}s...")

                # Attendre par petits intervalles pour pouvoir s'arrÃªter rapidement
                for _ in range(check_interval):
                    if not running:
                        break
                    await asyncio.sleep(1)

        except Exception as e:
            logger.error(f"âŒ Erreur dans la boucle du daemon: {e}")
            import traceback
            logger.error(traceback.format_exc())

            # Attendre avant de rÃ©essayer
            if running:
                logger.info("â³ Attente de 30s avant de rÃ©essayer...")
                await asyncio.sleep(30)

    # Statistiques finales
    total_duration = datetime.now() - start_time
    logger.info("\n" + "=" * 70)
    logger.info("ğŸ›‘ ARRÃŠT DU DAEMON")
    logger.info("=" * 70)
    logger.info(f"   DurÃ©e totale: {total_duration}")
    logger.info(f"   Sites traitÃ©s: {total_processed}")
    logger.info("=" * 70)


def main():
    """Point d'entrÃ©e principal"""
    import argparse

    parser = argparse.ArgumentParser(
        description="Daemon TURBO de recherche d'emails valides en continu"
    )
    parser.add_argument('--check-interval', type=int, default=60,
                        help='Intervalle entre les vÃ©rifications en secondes (dÃ©faut: 60)')
    parser.add_argument('--batch-size', type=int, default=100,
                        help='Taille des lots (dÃ©faut: 100)')
    parser.add_argument('--limit-per-run', type=int, default=2000,
                        help='Nombre max de sites par exÃ©cution (dÃ©faut: 2000)')
    parser.add_argument('--concurrent', type=int, default=100,
                        help='Nombre max de requÃªtes HTTP simultanÃ©es (dÃ©faut: 100)')
    parser.add_argument('--parallel-sites', type=int, default=20,
                        help='Nombre de sites traitÃ©s en parallÃ¨le (dÃ©faut: 20)')

    args = parser.parse_args()

    asyncio.run(daemon_loop(
        check_interval=args.check_interval,
        batch_size=args.batch_size,
        limit_per_run=args.limit_per_run,
        max_concurrent=args.concurrent,
        parallel_sites=args.parallel_sites
    ))


if __name__ == "__main__":
    main()
