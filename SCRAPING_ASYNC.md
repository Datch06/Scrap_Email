# ðŸš€ Scraping Asynchrone - Guide Complet

## Vue d'ensemble

Le systÃ¨me de scraping asynchrone utilise `asyncio` et `aiohttp` pour scraper les sites **4-5x plus rapidement** que les scrapers synchrones traditionnels.

### Nouveaux outils disponibles

1. **scrape_async_linkavista.py** - Scraper asynchrone LinkAvista
2. **email_finder_async.py** - Module de recherche d'emails avancÃ©
3. **rescrape_no_emails_async.py** - Re-scraper pour sites sans emails

---

## ðŸ“Š Performances

### Comparaison Sync vs Async

| MÃ©trique | Synchrone | Asynchrone | Gain |
|----------|-----------|------------|------|
| Sites/minute | ~500 | ~2000+ | 4x |
| Pages vÃ©rifiÃ©es/domaine | 3-5 | 8-10 | 2x |
| Taux de dÃ©couverte emails | 10-15% | 20-30% | 2x |
| Temps pour 1000 sites | ~30 min | ~6-8 min | 4x |

---

## ðŸ”§ Installation

### DÃ©pendances

```bash
pip3 install aiohttp aiofiles
```

VÃ©rifier l'installation :
```bash
python3 -c "import aiohttp; print('âœ… aiohttp installÃ©')"
```

---

## 1ï¸âƒ£ Scraper Asynchrone LinkAvista

### Description

Scrape LinkAvista MarketLink de maniÃ¨re asynchrone avec tous les filtres (Normal, Sensitive, Google News).

### Utilisation

```bash
cd /var/www/Scrap_Email
python3 scrape_async_linkavista.py
```

### Configuration

Modifier les paramÃ¨tres dans `scrape_async_linkavista.py` :

```python
EMAIL = "votre_email@linkavista.com"
PASSWORD = "votre_password"
MAX_CONCURRENT = 50      # RequÃªtes simultanÃ©es (recommandÃ©: 30-70)
BATCH_SIZE = 100         # Taille des lots (recommandÃ©: 50-150)
MAX_PAGES = 100          # Pages par filtre (recommandÃ©: 50-100)
```

### FonctionnalitÃ©s

- âœ… **Extraction asynchrone** de tous les domaines LinkAvista
- âœ… **3 filtres** combinÃ©s (Normal + Sensitive + Google News)
- âœ… **Recherche d'emails avancÃ©e** sur 8-10 pages par site
- âœ… **DÃ©tection d'emails obfusquÃ©s** (contact [at] domain [dot] com)
- âœ… **Protection .gouv.fr** intÃ©grÃ©e
- âœ… **Gestion des doublons** automatique
- âœ… **Statistiques en temps rÃ©el**

### Exemple de sortie

```
ðŸš€ SCRAPING LINKAVISTA ASYNCHRONE - ULTRA RAPIDE
================================================================================
   Concurrence: 50 requÃªtes simultanÃ©es
   Batch size: 100 domaines par lot
================================================================================

ðŸ“¥ PHASE 1: Extraction ASYNCHRONE de tous les domaines
================================================================================

ðŸ” Filtre: Normal
--------------------------------------------------------------------------------
âš¡ Extraction de 100 pages en parallÃ¨le...
ðŸ“„ Page   1/100 â†’ 156 sites (+156 nouveaux) | Total: 156
ðŸ“„ Page   2/100 â†’ 142 sites (+89 nouveaux) | Total: 245
[...]
âœ… Normal: +12,458 domaines supplÃ©mentaires

ðŸ” Filtre: Sensitive
--------------------------------------------------------------------------------
[...]
âœ… Sensitive: +2,847 domaines supplÃ©mentaires

ðŸŽ¯ TOTAL FINAL: 15,305 domaines uniques extraits

â±ï¸  Temps d'extraction: 125.3s (122.1 domaines/sec)

ðŸ“§ PHASE 2: Recherche d'emails ASYNCHRONE et ajout en base
================================================================================

ðŸ”„ Traitement du lot 1/154 (100 domaines)...
âœ… Lot traitÃ© en 8.2s (12.2 sites/sec)
   AjoutÃ©s: 87 | IgnorÃ©s: 13 | Emails: 24

[...]

================================================================================
âœ… SCRAPING ASYNCHRONE TERMINÃ‰!
================================================================================
   Temps total: 432.5s (7.2 minutes)
   Domaines extraits: 15,305
   Sites ajoutÃ©s: 12,458
   Sites ignorÃ©s: 2,847
   Emails trouvÃ©s: 3,247
   Taux de dÃ©couverte: 26.1%
   Vitesse moyenne: 35.4 domaines/sec
   Gain de performance: ~4-5x plus rapide que le scraper synchrone
================================================================================
```

---

## 2ï¸âƒ£ Module de Recherche d'Emails AvancÃ©

### Description

Module rÃ©utilisable pour chercher des emails sur n'importe quel domaine avec des techniques avancÃ©es.

### Utilisation Standalone

```python
from email_finder_async import find_emails_async
import asyncio

async def main():
    domains = ["example.com", "github.com", "stackoverflow.com"]
    results = await find_emails_async(
        domains,
        max_concurrent=50,
        max_pages_per_domain=10
    )

    for domain, emails in results.items():
        print(f"{domain}: {emails or 'Aucun email'}")

asyncio.run(main())
```

### Utilisation avec Session

```python
from email_finder_async import AsyncEmailFinder
import aiohttp
import asyncio

async def main():
    async with aiohttp.ClientSession() as session:
        finder = AsyncEmailFinder(session)
        emails = await finder.search_emails_on_domain("example.com", max_pages=10)
        print(f"Emails trouvÃ©s: {emails}")

asyncio.run(main())
```

### FonctionnalitÃ©s AvancÃ©es

#### 1. Pages vÃ©rifiÃ©es (25+ URLs par domaine)

- `/` (racine)
- `/contact`, `/contact-us`, `/contactez-nous`
- `/mentions-legales`, `/legal`, `/legal-notice`
- `/a-propos`, `/about`, `/about-us`
- `/imprint`, `/impressum` (sites DE/CH)
- `/equipe`, `/team`
- `/services`, `/nos-services`
- Versions `www.` de toutes les URLs

#### 2. DÃ©tection d'emails

- âœ… Pattern standard : `email@domain.com`
- âœ… Pattern mailto : `mailto:email@domain.com`
- âœ… Pattern obfusquÃ© : `contact [at] domain [dot] com`
- âœ… Pattern obfusquÃ© : `contact(at)domain(dot)com`

#### 3. Filtrage intelligent

Emails ignorÃ©s :
- Emails de test (`example@example.com`, `test@test.com`)
- Emails gÃ©nÃ©riques (`noreply@`, `admin@`, `webmaster@`)
- Emails avec mots-clÃ©s spam (`wix`, `wordpress`, `gravatar`, `sentry`)

---

## 3ï¸âƒ£ Re-scraper pour Sites Sans Emails

### Description

Re-scrape les sites oÃ¹ aucun email n'a Ã©tÃ© trouvÃ© pour maximiser la couverture.

### Utilisation

**Test sur 100 sites :**
```bash
cd /var/www/Scrap_Email
python3 rescrape_no_emails_async.py --limit 100
```

**Re-scraper tous les sites sans emails :**
```bash
python3 rescrape_no_emails_async.py
```

**Options avancÃ©es :**
```bash
# 500 sites, 40 requÃªtes simultanÃ©es, lots de 75
python3 rescrape_no_emails_async.py --limit 500 --concurrent 40 --batch-size 75
```

### Options

| Option | Description | DÃ©faut |
|--------|-------------|--------|
| `--limit` | Nombre max de sites Ã  traiter | Aucun (tous) |
| `--concurrent` | RequÃªtes simultanÃ©es | 30 |
| `--batch-size` | Taille des lots | 50 |

### Cas d'usage

1. **Maximiser la couverture** : Re-scraper pÃ©riodiquement les sites sans emails
2. **Sites avec emails temporaires** : Les sites peuvent ajouter des contacts aprÃ¨s
3. **AmÃ©lioration continue** : Le finder avancÃ© trouve plus d'emails

### Exemple de sortie

```
ðŸ”„ RE-SCRAPING ASYNCHRONE DES SITES SANS EMAILS
================================================================================
   Concurrence: 30 requÃªtes simultanÃ©es
   Batch size: 50 sites par lot
   Limite: 1000 sites
================================================================================

ðŸ“Š Sites Ã  re-scraper: 1,000

ðŸ”„ Lot 1/20 (50 sites)
--------------------------------------------------------------------------------
âœ… example.com                                     â†’ contact@example.com
âŒ test-site.fr                                    â†’ Toujours aucun email
âœ… another-domain.com                              â†’ info@another-domain.com; sales@another-domain.com
[...]

â±ï¸  Lot traitÃ© en 12.4s (4.0 sites/sec)
   Emails trouvÃ©s dans ce lot: 12

[...]

================================================================================
âœ… RE-SCRAPING TERMINÃ‰!
================================================================================
   Temps total: 245.8s (4.1 minutes)
   Sites re-scrapÃ©s: 1,000
   Emails trouvÃ©s: 234 (23.4%)
   Toujours sans email: 766
   Vitesse moyenne: 4.1 sites/sec
================================================================================

ðŸ’¡ Gain estimÃ©: 234 nouveaux contacts !
ðŸŽ¯ Consultez l'admin: https://admin.perfect-cocon-seo.fr
```

---

## ðŸŽ¯ Workflow RecommandÃ©

### 1. Scraping Initial

```bash
# Scraper LinkAvista de maniÃ¨re asynchrone
python3 scrape_async_linkavista.py
```

**RÃ©sultat attendu :**
- 15,000+ domaines extraits
- 3,000-4,000 emails trouvÃ©s (~25%)
- Temps : 6-10 minutes

### 2. Re-scraping des Sites Sans Emails

Attendre 1-2 jours, puis :

```bash
# Re-scraper 1000 sites sans emails pour tester
python3 rescrape_no_emails_async.py --limit 1000
```

**RÃ©sultat attendu :**
- 200-300 emails supplÃ©mentaires trouvÃ©s (~20-30%)
- Temps : 4-6 minutes

Si les rÃ©sultats sont bons, re-scraper tous les sites :

```bash
# Re-scraper TOUS les sites sans emails
python3 rescrape_no_emails_async.py
```

### 3. Re-scraping PÃ©riodique

Programmer un re-scraping mensuel :

```bash
# Crontab : tous les 1er du mois Ã  3h du matin
0 3 1 * * cd /var/www/Scrap_Email && python3 rescrape_no_emails_async.py --limit 5000 >> /var/log/rescrape.log 2>&1
```

---

## âš™ï¸ Optimisation des Performances

### Ajuster la Concurrence

**Trop lent ?** Augmenter `max_concurrent` :
```python
MAX_CONCURRENT = 70  # Au lieu de 50
```

**Trop d'erreurs/timeouts ?** Diminuer `max_concurrent` :
```python
MAX_CONCURRENT = 30  # Au lieu de 50
```

### Ajuster les Timeouts

Dans `email_finder_async.py` :
```python
# Timeout par page (dÃ©faut: 5s)
async with self.session.get(url, timeout=aiohttp.ClientTimeout(total=5)) as response:
```

Augmenter Ã  8s pour les sites lents :
```python
timeout=aiohttp.ClientTimeout(total=8)
```

### Monitoring des Performances

Observer les statistiques en temps rÃ©el :
- **Sites/sec** : doit Ãªtre > 10 en moyenne
- **Taux de dÃ©couverte** : doit Ãªtre > 20%
- **Erreurs/timeouts** : doit Ãªtre < 5%

---

## ðŸ› Troubleshooting

### "Too many open files"

Augmenter la limite systÃ¨me :
```bash
ulimit -n 4096
```

Ou diminuer `max_concurrent` Ã  20-30.

### "SSL: CERTIFICATE_VERIFY_FAILED"

DÃ©jÃ  gÃ©rÃ© avec `ssl=False` dans les connecteurs.

### "Connection timeout"

Sites trop lents. Augmenter le timeout ou ignorer ces domaines.

### Base de donnÃ©es locked

SQLite peut avoir des problÃ¨mes avec trop de writes concurrents.
Solution : Traiter par lots plus petits (`batch_size=25`).

---

## ðŸ“ˆ Statistiques Actuelles

**Avant scraping asynchrone :**
- Sites : 79,430
- Emails trouvÃ©s : ~12,000 (15%)

**AprÃ¨s scraping asynchrone (estimation) :**
- Sites : 95,000+ (+20%)
- Emails trouvÃ©s : 25,000+ (+110%)
- Taux de dÃ©couverte : 26%

---

## ðŸŽ“ Pour Aller Plus Loin

### Ajouter de Nouvelles Sources

CrÃ©er `scrape_async_[source].py` en s'inspirant de `scrape_async_linkavista.py`.

### AmÃ©liorer la DÃ©tection d'Emails

Modifier `email_finder_async.py` pour :
- Ajouter des patterns d'emails
- VÃ©rifier plus de pages
- Extraire des footers HTML
- Parser les rÃ©seaux sociaux

### ParallÃ©liser le Re-scraping

Lancer plusieurs instances en parallÃ¨le avec des limites diffÃ©rentes :

```bash
# Terminal 1
python3 rescrape_no_emails_async.py --limit 5000 &

# Terminal 2
python3 rescrape_no_emails_async.py --limit 5000 --offset 5000 &
```

(NÃ©cessite d'ajouter `--offset` dans le script)

---

## ðŸ“ž Support

Pour toute question : david@somucom.com

**Documentation connexe :**
- [README.md](README.md) - Vue d'ensemble du projet
- [GUIDE_SCRAPING_TEMPS_REEL.md](GUIDE_SCRAPING_TEMPS_REEL.md) - Scraping temps rÃ©el
- [VALIDATION_EMAILS.md](VALIDATION_EMAILS.md) - Validation des emails

---

**Built with â¤ï¸ using Python asyncio + aiohttp**
