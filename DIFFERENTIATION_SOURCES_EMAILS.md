# Diff√©renciation des Sources d'Emails

Date: 2025-10-18

---

## Objectif

Distinguer dans la base de donn√©es et l'interface les emails trouv√©s par:
1. **Scraping direct** du site web (Feuille 1)
2. **Informations SIRET/SIREN** (Feuille 3)

---

## Modifications Effectu√©es

### 1. Base de Donn√©es

#### Nouvelle Colonne ajout√©e
```sql
ALTER TABLE sites
ADD COLUMN email_source VARCHAR(20);
```

**Valeurs possibles**:
- `'scraping'` - Email trouv√© par scraping du site web
- `'siret'` - Email trouv√© via les informations SIRET/SIREN

#### Migration
‚úÖ Script cr√©√©: [migrate_add_email_source.py](migrate_add_email_source.py:1)
‚úÖ Migration ex√©cut√©e: 51 sites mis √† jour avec `email_source='scraping'`

### 2. Mod√®le de Donn√©es

#### Fichier: [database.py](database.py:45)
```python
email_source = Column(String(20), nullable=True)  # "scraping" ou "siret"
```

Le champ est maintenant inclus dans:
- Le mod√®le `Site`
- La m√©thode `to_dict()` pour l'API

### 3. Helper Database

#### Fichier: [db_helper.py](db_helper.py:40)
```python
def update_email(self, domain, emails, email_source='scraping'):
    """
    Mettre √† jour les emails d'un site

    Args:
        domain: Le nom de domaine
        emails: Les emails trouv√©s
        email_source: Source de l'email ('scraping' ou 'siret')
    """
```

### 4. API REST

#### Route: `/api/stats`
Nouvelles statistiques ajout√©es:
- `emails_from_scraping` - Nombre d'emails trouv√©s par scraping
- `emails_from_siret` - Nombre d'emails trouv√©s via SIRET

#### Route: `/api/sites/<id>` (PUT)
Support du champ `email_source` lors de la mise √† jour.

#### Route: `/api/sites`
Le champ `email_source` est maintenant inclus dans les r√©ponses JSON.

### 5. Scripts d'Import

#### Nouveau Script: [import_feuille3_emails.py](import_feuille3_emails.py:1)
- Importe les emails depuis la Feuille 3 (Google Sheets)
- Marque automatiquement `email_source='siret'`
- Ne remplace PAS les emails trouv√©s par scraping
- Logique de priorit√©: scraping > siret

**Utilisation**:
```bash
python3 import_feuille3_emails.py
```

---

## Statistiques Actuelles

### R√©partition des Emails

| Source | Nombre | Pourcentage |
|--------|--------|-------------|
| **Scraping** | 51 | 100% |
| **SIRET** | 0 | 0% |
| **Total** | 51 | 1.8% des sites |

### √âtat Global

- **Total de sites**: 2,841
- **Sites avec email**: 51 (1.8%)
  - Depuis scraping: 51
  - Depuis SIRET: 0
- **Sites avec SIRET**: 810 (28.5%)
- **Sites avec dirigeants**: 64 (2.3%)
- **Sites complets**: 1 (0.0%)

---

## Structure des Google Sheets

### Feuille 1
Contient les sites avec leurs emails trouv√©s par **scraping direct**:
- Colonne: Site/Domain
- Colonne: Emails

### Feuille 3
Contient les sites avec SIRET et dirigeants:
- Colonne: Domaine
- Colonne: SIRET/SIREN
- Colonne: Dirigeants
- Colonne: Source

**Note**: La Feuille 3 actuelle ne contient **PAS** de colonne "Emails".

Pour utiliser cette fonctionnalit√©, il faudrait:
1. Soit ajouter une colonne "Emails" dans la Feuille 3
2. Soit r√©cup√©rer les emails via l'API SIRET/SIREN et les ajouter

---

## Utilisation de l'API

### Obtenir les Statistiques par Source

```bash
curl -s https://admin.perfect-cocon-seo.fr/api/stats | python3 -m json.tool
```

**R√©ponse**:
```json
{
  "total_sites": 2841,
  "sites_with_email": 51,
  "emails_from_scraping": 51,
  "emails_from_siret": 0,
  ...
}
```

### Filtrer les Sites par Source d'Email

**Sites avec emails depuis le scraping**:
```bash
curl "https://admin.perfect-cocon-seo.fr/api/sites?has_email=true" \
  | python3 -c "import sys, json; data=json.load(sys.stdin); [print(f'{s[\"domain\"]}: source={s[\"email_source\"]}') for s in data['sites'] if s.get('email_source') == 'scraping']"
```

**Sites avec emails depuis SIRET** (quand disponibles):
```bash
curl "https://admin.perfect-cocon-seo.fr/api/sites?has_email=true" \
  | python3 -c "import sys, json; data=json.load(sys.stdin); [print(f'{s[\"domain\"]}: source={s[\"email_source\"]}') for s in data['sites'] if s.get('email_source') == 'siret']"
```

---

## Logique de Priorit√©

Lorsqu'un site a d√©j√† un email:

1. **Email existant = scraping** ‚Üí Conserv√©, pas de remplacement
2. **Email existant = siret** ‚Üí Peut √™tre remplac√© par un email scraping
3. **Pas d'email** ‚Üí Accepte email de n'importe quelle source

**Justification**: Les emails trouv√©s par scraping direct du site sont g√©n√©ralement plus fiables car ils sont directement affich√©s sur le site web.

---

## Prochaines √âtapes

### 1. Enrichir la Feuille 3 avec des Emails

Si vous souhaitez ajouter des emails trouv√©s via SIRET:

a. **Manuellement dans Google Sheets**:
   - Ajouter une colonne "Emails" dans la Feuille 3
   - Remplir avec les emails trouv√©s via les API SIRET

b. **Via un Script Python**:
   - Cr√©er un script pour r√©cup√©rer les emails depuis l'API Pappers/Infogreffe
   - Utiliser le SIRET pour trouver les contacts

### 2. Afficher la Source dans l'Interface Web

Modifier les templates HTML pour afficher un badge indiquant la source:
- üåê Badge "Scraping" pour les emails trouv√©s sur le site
- üè¢ Badge "SIRET" pour les emails trouv√©s via les infos l√©gales

### 3. Export CSV avec Source

Le fichier CSV export√© inclut maintenant le champ `email_source`:

```bash
curl -o sites.csv https://admin.perfect-cocon-seo.fr/api/export/csv
```

Colonnes:
- ID, Domaine, Statut, **Emails**, **Email_Source**, SIRET, SIREN, Dirigeants...

---

## Commandes Utiles

### V√©rifier la R√©partition

```bash
# Compter les emails par source
python3 -c "
from database import get_session, Site
session = get_session()

scraping = session.query(Site).filter(
    Site.emails.isnot(None),
    Site.emails != '',
    Site.emails != 'NO EMAIL FOUND',
    Site.email_source == 'scraping'
).count()

siret = session.query(Site).filter(
    Site.emails.isnot(None),
    Site.emails != '',
    Site.emails != 'NO EMAIL FOUND',
    Site.email_source == 'siret'
).count()

print(f'Emails depuis scraping: {scraping}')
print(f'Emails depuis SIRET: {siret}')
print(f'Total: {scraping + siret}')

session.close()
"
```

### Exemples de Sites

```bash
# Sites avec email depuis scraping
python3 -c "
from database import get_session, Site
session = get_session()
sites = session.query(Site).filter(Site.email_source == 'scraping').limit(5).all()
for site in sites:
    print(f'{site.domain}: {site.emails} (source: {site.email_source})')
session.close()
"
```

---

## R√©sum√©

‚úÖ **Base de donn√©es**: Colonne `email_source` ajout√©e
‚úÖ **API**: Statistiques par source disponibles
‚úÖ **Scripts**: Import avec diff√©renciation des sources
‚úÖ **Migration**: Donn√©es existantes marqu√©es comme "scraping"
‚úÖ **Documentation**: Compl√®te et √† jour

**Acc√®s**: https://admin.perfect-cocon-seo.fr/api/stats

---

## Notes Techniques

### Sch√©ma de la Base de Donn√©es

```
Table: sites
‚îú‚îÄ‚îÄ id (INTEGER)
‚îú‚îÄ‚îÄ domain (STRING)
‚îú‚îÄ‚îÄ emails (TEXT)
‚îú‚îÄ‚îÄ email_source (STRING) ‚Üê NOUVEAU
‚îÇ   ‚îú‚îÄ‚îÄ 'scraping' - Email trouv√© par scraping
‚îÇ   ‚îî‚îÄ‚îÄ 'siret' - Email trouv√© via SIRET
‚îú‚îÄ‚îÄ email_checked (BOOLEAN)
‚îú‚îÄ‚îÄ email_found_at (DATETIME)
‚îú‚îÄ‚îÄ siret (STRING)
‚îú‚îÄ‚îÄ siren (STRING)
‚îú‚îÄ‚îÄ leaders (TEXT)
‚îî‚îÄ‚îÄ ...
```

### Exemples de Requ√™tes SQL

```sql
-- Emails par source
SELECT email_source, COUNT(*) as count
FROM sites
WHERE emails IS NOT NULL
  AND emails != ''
  AND emails != 'NO EMAIL FOUND'
GROUP BY email_source;

-- Sites sans email_source d√©fini
SELECT domain, emails
FROM sites
WHERE emails IS NOT NULL
  AND emails != 'NO EMAIL FOUND'
  AND email_source IS NULL;
```

---

**Syst√®me pr√™t √† diff√©rencier les sources d'emails !** üéâ

Pour importer des emails depuis la Feuille 3, ajoutez d'abord une colonne "Emails" dans le Google Sheet, puis relancez le script d'import.
