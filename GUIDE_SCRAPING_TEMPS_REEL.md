# âš¡ Guide du Scraping EN TEMPS RÃ‰EL

## ğŸ¯ Nouveau Script: scrape_realtime_complete.py

**Le script le plus avancÃ©** avec upload instantanÃ© et recherche simultanÃ©e!

---

## âœ¨ FonctionnalitÃ©s

### ğŸ”¥ Upload EN TEMPS RÃ‰EL
- âœ… Chaque site est **immÃ©diatement visible** dans l'admin
- âœ… Pas besoin d'attendre la fin du scraping
- âœ… Commit instantanÃ© aprÃ¨s chaque site

### ğŸ“§ Recherche SIMULTANÃ‰E
Pour chaque site dÃ©couvert, le script recherche **automatiquement**:

1. **âœ‰ï¸ EMAILS** (5 pages par site)
   - Page d'accueil
   - /contact
   - /contact-us
   - /mentions-legales
   - /qui-sommes-nous

2. **ğŸ¢ SIRET/SIREN** (7 pages lÃ©gales)
   - /mentions-legales
   - /mentions-legales.html
   - /mentions_legales
   - /mentions
   - /legal
   - /a-propos
   - /about

3. **ğŸ’¾ Upload instantanÃ©** vers la base de donnÃ©es
   - Visible immÃ©diatement sur https://admin.perfect-cocon-seo.fr

---

## ğŸš€ Utilisation

### Lancement Simple

```bash
cd /var/www/Scrap_Email

# Lancer le scraping temps rÃ©el
python3 scrape_realtime_complete.py
```

**Sortie:**
```
================================================================================
ğŸš€ SCRAPING TEMPS RÃ‰EL - UPLOAD INSTANTANÃ‰ DANS L'ADMIN
================================================================================

âš¡ Recherche simultanÃ©e:
   - âœ‰ï¸  Emails
   - ğŸ¢ SIRET/SIREN
   - ğŸ“Š Upload instantanÃ© vers admin

Configuration:
   Pages/site vendeur: 500
   Profondeur: 5
   Pause sites: 0.1s

ğŸ¯ Consultez l'admin en temps rÃ©el sur:
   https://admin.perfect-cocon-seo.fr

================================================================================
CYCLE #1 - 2025-10-18 16:00:00
================================================================================

ğŸ“Š Progression:
   Total sites vendeurs: 75354
   DÃ©jÃ  explorÃ©s: 0
   Restants: 75354

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[1/75354] Site vendeur: https://example-backlinks.fr
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  ğŸ” Crawling https://example-backlinks.fr...

    [1] site1.fr âœ‰ï¸ contact@site1.fr... ğŸ¢ SIRET:12345678... âœ…
    [2] site2.fr âœ‰ï¸ âœ— ğŸ¢ SIREN:123456789 âœ…
    [3] site3.fr âœ‰ï¸ info@site3.fr; hello@site3.fr... ğŸ¢ âœ— âœ…
    ...

  ğŸ“Š RÃ©sultats pour https://example-backlinks.fr:
     Domaines: 342 | Emails: 156 | SIRET: 98

ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥
ğŸ“ˆ STATISTIQUES GLOBALES (Cycle #1)
ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥
   Total sites en base: 15432
   Avec email: 6234 (40.4%)
   Avec SIRET: 4321 (28.0%)

   Ce cycle:
   Domaines trouvÃ©s: 12582
   Emails trouvÃ©s: 5052
   SIRET trouvÃ©s: 3521
ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥
```

### Lancement en ArriÃ¨re-Plan (24/7)

```bash
cd /var/www/Scrap_Email

# Lancer en mode daemon
nohup python3 scrape_realtime_complete.py > scraping_realtime.log 2>&1 &

# Sauvegarder le PID
echo $! > scraping_realtime.pid

# Suivre les logs en temps rÃ©el
tail -f scraping_realtime.log

# Voir uniquement les sites trouvÃ©s
tail -f scraping_realtime.log | grep "âœ…"

# Voir uniquement les statistiques
tail -f scraping_realtime.log | grep "ğŸ“ˆ"
```

### ArrÃªter Proprement

```bash
# MÃ©thode 1: Ctrl+C si en mode interactif

# MÃ©thode 2: Si en arriÃ¨re-plan
kill -SIGINT $(cat scraping_realtime.pid)

# VÃ©rifier que c'est arrÃªtÃ©
ps aux | grep scrape_realtime
```

---

## ğŸ“Š Monitoring en Temps RÃ©el

### Via l'Interface Admin

**Ouvrez dans votre navigateur:**
https://admin.perfect-cocon-seo.fr

Vous verrez les nouveaux sites apparaÃ®tre **en temps rÃ©el** pendant que le script tourne!

**RafraÃ®chir la page** toutes les 10-30 secondes pour voir les nouveaux sites.

### Via l'API

```bash
# Stats globales (actualisation instantanÃ©e)
watch -n 5 'curl -s https://admin.perfect-cocon-seo.fr/api/stats | python3 -m json.tool'

# Affichage formatÃ©
watch -n 5 'curl -s https://admin.perfect-cocon-seo.fr/api/stats | python3 -c "
import sys, json
data = json.load(sys.stdin)
print(f'''
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     STATISTIQUES TEMPS RÃ‰EL            â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Total sites: {data[\"total_sites\"]:>23} â•‘
â•‘ Avec email:  {data[\"sites_with_email\"]:>23} â•‘
â•‘ Taux:        {data[\"email_rate\"]:>21}% â•‘
â•‘ Avec SIRET:  {data[\"sites_with_siret\"]:>23} â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
''')
"'
```

### Via les Logs

```bash
# Compter les sites ajoutÃ©s
grep -c "âœ…" scraping_realtime.log

# Compter les emails trouvÃ©s
grep "âœ‰ï¸" scraping_realtime.log | grep -v "âœ—" | wc -l

# Compter les SIRET trouvÃ©s
grep "ğŸ¢" scraping_realtime.log | grep -v "âœ—" | wc -l

# DerniÃ¨res 20 entrÃ©es
tail -20 scraping_realtime.log

# Stats uniquement
grep "ğŸ“ˆ" scraping_realtime.log | tail -5
```

---

## ğŸ¯ Avantages vs Ancienne Version

### âœ… scrape_realtime_complete.py (NOUVEAU)

- âš¡ **Upload INSTANTANÃ‰** dans l'admin
- ğŸ“§ Recherche **EMAIL** automatique
- ğŸ¢ Recherche **SIRET/SIREN** automatique
- ğŸ“Š Visible **immÃ©diatement** dans l'interface
- ğŸ”„ Commit aprÃ¨s **chaque site**
- ğŸ“ˆ Stats en temps rÃ©el tous les 50 sites

### â³ scrape_backlinks_infinite.py (Ancien)

- ğŸ’¾ Upload par batch
- ğŸ“§ Email uniquement
- âŒ Pas de SIRET
- â° Visible Ã  la fin du batch
- ğŸ“Š Stats de fin de cycle

---

## ğŸ”§ Configuration

### Ajuster la Vitesse

Ã‰diter [scrape_realtime_complete.py](/var/www/Scrap_Email/scrape_realtime_complete.py):

```python
# Ligne ~21-23
PAUSE_BETWEEN_SITES = 0.1    # Pause entre chaque site acheteur
PAUSE_BETWEEN_PAGES = 0.05   # Pause entre chaque page

# Pour aller plus vite (risque de blocage)
PAUSE_BETWEEN_SITES = 0.05
PAUSE_BETWEEN_PAGES = 0.02

# Pour aller plus lent (plus sÃ»r)
PAUSE_BETWEEN_SITES = 0.5
PAUSE_BETWEEN_PAGES = 0.2
```

### Ajuster les Pages CrawlÃ©es

```python
# Ligne ~26-27
MAX_PAGES_PER_SELLER_SITE = 500  # Pages max par site vendeur
MAX_DEPTH = 5                     # Profondeur max

# Pour plus de domaines (plus lent)
MAX_PAGES_PER_SELLER_SITE = 1000
MAX_DEPTH = 7

# Pour aller plus vite (moins de domaines)
MAX_PAGES_PER_SELLER_SITE = 200
MAX_DEPTH = 3
```

### Ajuster l'Affichage des Stats

```python
# Ligne ~30
STATS_INTERVAL = 50  # Afficher stats tous les 50 sites

# Plus frÃ©quent
STATS_INTERVAL = 10

# Moins frÃ©quent
STATS_INTERVAL = 100
```

---

## ğŸ“ˆ Estimations de Performance

### Avec 75,354 Sites Vendeurs

**Par site vendeur:**
- Pages crawlÃ©es: ~500
- Domaines trouvÃ©s: ~300-400
- Emails trouvÃ©s: ~120-160 (40%)
- SIRET trouvÃ©s: ~84-112 (28%)

**Total estimÃ©:**
- **Domaines**: 75,354 Ã— 350 = **~26 millions**
- **Emails**: 26M Ã— 40% = **~10.4 millions**
- **SIRET**: 26M Ã— 28% = **~7.3 millions**

### Temps EstimÃ© (24/7)

**Par domaine:**
- Recherche email: 5 pages Ã— 0.05s = ~0.25s
- Recherche SIRET: 7 pages Ã— 0.05s = ~0.35s
- Upload + pauses: ~0.1s
- **Total**: ~0.7s par domaine

**Temps total:**
- 26 millions Ã— 0.7s = 18.2 millions secondes
- = 304,000 minutes
- = 5,066 heures
- = **~211 jours** (7 mois en continu 24/7)

**En pratique avec optimisations:**
- GrÃ¢ce aux caches et skips: **~3-4 mois**

---

## ğŸ¯ Commandes Rapides

### Statistiques en Direct

```bash
# Stats base de donnÃ©es
sqlite3 /var/www/Scrap_Email/scrap_email.db "
SELECT
  COUNT(*) as total,
  SUM(CASE WHEN emails IS NOT NULL AND emails != 'NO EMAIL FOUND' THEN 1 ELSE 0 END) as with_email,
  SUM(CASE WHEN siret IS NOT NULL AND siret != 'NON TROUVÃ‰' THEN 1 ELSE 0 END) as with_siret
FROM sites;
"

# Derniers sites ajoutÃ©s
sqlite3 /var/www/Scrap_Email/scrap_email.db "
SELECT domain, emails, siret
FROM sites
ORDER BY created_at DESC
LIMIT 10;
"

# Taux de succÃ¨s
sqlite3 /var/www/Scrap_Email/scrap_email.db "
SELECT
  ROUND(100.0 * SUM(CASE WHEN emails IS NOT NULL AND emails != 'NO EMAIL FOUND' THEN 1 ELSE 0 END) / COUNT(*), 2) || '%' as taux_email,
  ROUND(100.0 * SUM(CASE WHEN siret IS NOT NULL AND siret != 'NON TROUVÃ‰' THEN 1 ELSE 0 END) / COUNT(*), 2) || '%' as taux_siret
FROM sites;
"
```

### Performance du Scraping

```bash
# Sites par heure
echo "Sites ajoutÃ©s dans la derniÃ¨re heure:"
sqlite3 /var/www/Scrap_Email/scrap_email.db "
SELECT COUNT(*)
FROM sites
WHERE created_at > datetime('now', '-1 hour');
"

# Vitesse moyenne
echo "Vitesse moyenne (sites/minute):"
sqlite3 /var/www/Scrap_Email/scrap_email.db "
SELECT
  COUNT(*) / ((julianday('now') - julianday(MIN(created_at))) * 24 * 60) as sites_per_minute
FROM sites
WHERE created_at > datetime('now', '-1 day');
"
```

---

## ğŸ”¥ Service Systemd (Production)

Pour que le scraping redÃ©marre automatiquement en cas de crash ou reboot:

```bash
# CrÃ©er le service
sudo nano /etc/systemd/system/scrap-realtime.service
```

Contenu:
```ini
[Unit]
Description=Scraping Temps RÃ©el avec Upload InstantanÃ©
After=network.target

[Service]
Type=simple
User=www-data
WorkingDirectory=/var/www/Scrap_Email
Environment="PYTHONUNBUFFERED=1"
ExecStart=/usr/bin/python3 /var/www/Scrap_Email/scrape_realtime_complete.py
Restart=always
RestartSec=30
StandardOutput=append:/var/log/scrap-realtime.log
StandardError=append:/var/log/scrap-realtime.log

[Install]
WantedBy=multi-user.target
```

Activer:
```bash
sudo systemctl daemon-reload
sudo systemctl enable scrap-realtime.service
sudo systemctl start scrap-realtime.service

# VÃ©rifier
sudo systemctl status scrap-realtime.service

# Logs
sudo journalctl -u scrap-realtime.service -f
```

---

## ğŸ‰ RÃ©sultat Final Attendu

AprÃ¨s plusieurs semaines/mois de scraping continu 24/7:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘       RÃ‰SULTATS FINAUX ESTIMÃ‰S                 â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Total sites dÃ©couverts:    26,000,000         â•‘
â•‘ Avec email:                10,400,000 (40%)   â•‘
â•‘ Avec SIRET:                 7,300,000 (28%)   â•‘
â•‘ Complets (email+SIRET):     5,200,000 (20%)   â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Sites vendeurs crawlÃ©s:        75,354         â•‘
â•‘ Cycles complets:                    3         â•‘
â•‘ DurÃ©e totale:                  4 mois         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Vous aurez alors la BASE DE DONNÃ‰ES LA PLUS COMPLÃˆTE de tous les acheteurs de backlinks en France!** ğŸš€

---

## ğŸ“ Support

- **Interface Admin**: https://admin.perfect-cocon-seo.fr
- **API Stats**: https://admin.perfect-cocon-seo.fr/api/stats
- **Logs**: `tail -f scraping_realtime.log`
- **Base de donnÃ©es**: `/var/www/Scrap_Email/scrap_email.db`

---

**Bon scraping! âš¡**
