#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script pour scraper les liens externes du site fr.bijouxenvogue.com
Exclut les liens vers les rÃ©seaux sociaux
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re
from collections import defaultdict
from datetime import datetime

class BijouxEnVogueScraper:
    def __init__(self):
        self.base_url = "https://fr.bijouxenvogue.com"
        self.visited_urls = set()
        self.external_links = set()
        self.social_networks = {
            'facebook.com', 'fb.com', 'twitter.com', 'x.com', 'instagram.com',
            'linkedin.com', 'youtube.com', 'tiktok.com', 'pinterest.com',
            'snapchat.com', 'whatsapp.com', 'telegram.org', 'discord.com',
            'reddit.com', 'tumblr.com', 'flickr.com', 'vimeo.com',
            'dailymotion.com', 'twitch.tv', 'medium.com', 'github.com',
            'gitlab.com', 'bitbucket.org'
        }
        
    def is_social_network(self, url):
        """VÃ©rifie si l'URL pointe vers un rÃ©seau social"""
        domain = urlparse(url).netloc.lower()
        for social in self.social_networks:
            if social in domain:
                return True
        return False
    
    def is_external_link(self, url, base_domain):
        """VÃ©rifie si l'URL est externe au site"""
        parsed_url = urlparse(url)
        return parsed_url.netloc and parsed_url.netloc != base_domain
    
    def get_page_links(self, url):
        """RÃ©cupÃ¨re tous les liens d'une page"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            links = []
            
            # RÃ©cupÃ©rer tous les liens href
            for link in soup.find_all('a', href=True):
                href = link['href']
                full_url = urljoin(url, href)
                links.append(full_url)
            
            return links
            
        except Exception as e:
            print(f"Erreur lors de la rÃ©cupÃ©ration de {url}: {e}")
            return []
    
    def scrape_site(self, max_pages=None):
        """Scrape le site pour trouver tous les liens externes"""
        base_domain = urlparse(self.base_url).netloc
        urls_to_visit = [self.base_url]
        
        print(f"ğŸ” DÃ©but du scraping complet de {self.base_url}")
        if max_pages:
            print(f"ğŸ“Š Limite de pages: {max_pages}")
        else:
            print("ğŸ“Š Scraping complet - aucune limite de pages")
        print("-" * 50)
        
        page_count = 0
        
        while urls_to_visit:
            if max_pages and page_count >= max_pages:
                break
            current_url = urls_to_visit.pop(0)
            
            if current_url in self.visited_urls:
                continue
                
            self.visited_urls.add(current_url)
            page_count += 1
            
            print(f"ğŸ“„ Page {page_count}: {current_url}")
            
            # RÃ©cupÃ©rer les liens de la page
            links = self.get_page_links(current_url)
            
            for link in links:
                # Nettoyer l'URL
                clean_link = link.split('#')[0].split('?')[0]
                
                # VÃ©rifier si c'est un lien externe
                if self.is_external_link(clean_link, base_domain):
                    # VÃ©rifier si ce n'est pas un rÃ©seau social
                    if not self.is_social_network(clean_link):
                        self.external_links.add(clean_link)
                        print(f"  ğŸ”— Lien externe trouvÃ©: {clean_link}")
                
                # Ajouter les liens internes Ã  la liste de visite
                elif clean_link.startswith(self.base_url) and clean_link not in self.visited_urls and clean_link not in urls_to_visit:
                    # Filtrer les liens vers des fichiers (images, PDF, etc.)
                    if not any(clean_link.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.gif', '.pdf', '.doc', '.docx', '.zip', '.rar']):
                        urls_to_visit.append(clean_link)
        
        print("-" * 50)
        print(f"âœ… Scraping terminÃ©!")
        print(f"ğŸ“Š Pages visitÃ©es: {len(self.visited_urls)}")
        print(f"ğŸ”— Liens externes trouvÃ©s: {len(self.external_links)}")
    
    def categorize_links(self):
        """CatÃ©gorise les liens par domaine"""
        categories = defaultdict(list)
        
        for link in self.external_links:
            domain = urlparse(link).netloc
            categories[domain].append(link)
        
        return categories
    
    def save_results(self, filename="liens_externes_bijouxenvogue.txt"):
        """Sauvegarde les rÃ©sultats dans un fichier"""
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("LIENS EXTERNES DE FR.BIJOUXENVOGUE.COM\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"URL de base: {self.base_url}\n")
            f.write(f"Total de liens externes: {len(self.external_links)}\n\n")
            
            categories = self.categorize_links()
            
            for domain, links in sorted(categories.items()):
                f.write(f"\nğŸŒ DOMAINE: {domain}\n")
                f.write("-" * 30 + "\n")
                for link in sorted(links):
                    f.write(f"  â€¢ {link}\n")
        
        print(f"ğŸ’¾ RÃ©sultats sauvegardÃ©s dans: {filename}")
    
    def display_results(self):
        """Affiche les rÃ©sultats de maniÃ¨re organisÃ©e"""
        print("\n" + "="*60)
        print("ğŸ“‹ RÃ‰SULTATS DU SCRAPING")
        print("="*60)
        
        if not self.external_links:
            print("âŒ Aucun lien externe trouvÃ©")
            return
        
        categories = self.categorize_links()
        
        for domain, links in sorted(categories.items()):
            print(f"\nğŸŒ DOMAINE: {domain}")
            print("-" * 40)
            for link in sorted(links):
                print(f"  â€¢ {link}")
        
        print(f"\nğŸ“Š RÃ‰SUMÃ‰:")
        print(f"  â€¢ Total des liens externes: {len(self.external_links)}")
        print(f"  â€¢ Nombre de domaines uniques: {len(categories)}")

def main():
    scraper = BijouxEnVogueScraper()
    
    # Lancer le scraping complet (sans limite de pages)
    scraper.scrape_site()
    
    # Afficher les rÃ©sultats
    scraper.display_results()
    
    # Sauvegarder les rÃ©sultats
    scraper.save_results()
    
    return scraper.external_links

if __name__ == "__main__":
    external_links = main()
