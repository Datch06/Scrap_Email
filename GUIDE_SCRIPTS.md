# Guide des Scripts - Scrap_Email

## üìã Table des Mati√®res
- [Scripts Principaux](#scripts-principaux)
- [Scripts de Base de Donn√©es](#scripts-de-base-de-donn√©es)
- [Scripts d'Import/Export](#scripts-dimportexport)
- [Scripts de Scraping](#scripts-de-scraping)
- [Scripts Google Sheets](#scripts-google-sheets)
- [Scripts de Test](#scripts-de-test)
- [Scripts Utilitaires](#scripts-utilitaires)

---

## üöÄ Scripts Principaux

### **app.py**
**Interface web Flask pour g√©rer le scraping d'emails**
- Lance l'interface admin sur le port 5002
- API REST pour consulter/g√©rer les sites
- Endpoints principaux:
  - `GET /api/stats` - Statistiques globales
  - `GET /api/sites` - Liste des sites avec pagination
  - `GET /api/sites/<domain>` - D√©tails d'un site
  - `POST /api/scrape` - Lancer un scraping
- Accessible via: https://admin.perfect-cocon-seo.fr

**Usage:**
```bash
python3 app.py
# ou via systemd:
sudo systemctl start scrap-email-interface.service
```

### **wsgi.py**
**Point d'entr√©e WSGI pour d√©ploiement production**
- Utilis√© par Gunicorn ou autre serveur WSGI
- Configure l'application Flask pour production

---

## üíæ Scripts de Base de Donn√©es

### **database.py**
**Mod√®les SQLAlchemy de la base de donn√©es**
- D√©finit la structure de la table `sites`
- Champs principaux:
  - `domain` - Nom de domaine
  - `emails` - Emails trouv√©s (s√©par√©s par `;`)
  - `email_source` - Source: 'scraping' ou 'siret'
  - `siret` / `siren` - Identifiants entreprise
  - `leaders` - Dirigeants de l'entreprise
  - `status` - Statut du site (enum)
- Enums: `SiteStatus`, `ScrapingJobStatus`

### **db_helper.py**
**Helper pour faciliter les op√©rations en base**
- Context manager pour g√©rer les sessions
- M√©thodes principales:
  - `add_site(domain)` - Ajouter un site
  - `update_email(domain, emails, source)` - Mettre √† jour les emails
  - `update_siret(domain, siret, type)` - Mettre √† jour SIRET
  - `update_leaders(domain, leaders)` - Mettre √† jour dirigeants
  - `get_stats()` - Obtenir les statistiques

**Usage:**
```python
from db_helper import DBHelper

with DBHelper() as db:
    db.add_site('example.fr')
    db.update_email('example.fr', 'contact@example.fr', 'scraping')
```

### **migrate_add_email_source.py**
**Migration pour ajouter la colonne email_source**
- Ajoute la colonne `email_source` √† la table sites
- Marque les emails existants comme source='scraping'
- ‚ö†Ô∏è √Ä ex√©cuter une seule fois (d√©j√† fait)

---

## üì• Scripts d'Import/Export

### **import_feuille1_emails.py** ‚≠ê
**Importe les emails depuis Google Sheets Feuille 1**
- Source: Emails trouv√©s par scraping
- Structure du sheet (sans en-t√™te):
  - Colonne 0: Domain
  - Colonne 1: Emails (s√©par√©s par `;`)
  - Colonne 2: Date
  - Colonne 3: SIRET/SIREN
  - Colonne 5+: Dirigeants
- Marque tous les emails comme `email_source='scraping'`
- **R√©sultat**: 1182 emails import√©s

**Usage:**
```bash
python3 import_feuille1_emails.py
```

### **import_feuille3_emails.py**
**Importe les emails depuis Google Sheets Feuille 3**
- Source: Emails trouv√©s via recherche SIRET/SIREN
- Marque les emails comme `email_source='siret'`
- Ne remplace pas les emails d√©j√† trouv√©s par scraping

**Usage:**
```bash
python3 import_feuille3_emails.py
```

### **import_existing_data.py**
**Import initial depuis fichiers CSV et JSON**
- Importe depuis:
  - `emails_found.csv`
  - `emails_formatted.csv`
  - `emails_cleaned.csv`
  - `feuille1_results.json`
  - `feuille2_results.json`
  - `dirigeants_results.json`
  - Fichiers TXT de domaines
- ‚ö†Ô∏è Script historique, utiliser plut√¥t import_feuille1_emails.py

### **import_cleaned_emails.py**
**Importe depuis emails_cleaned.csv**
- Emails d√©j√† filtr√©s et format√©s
- Concat√®ne les emails par domaine
- Filtre les "NO EMAIL FOUND"

### **reimport_emails_improved.py**
**Version am√©lior√©e de l'import CSV**
- Groupe les emails par domaine
- Filtre les emails de tracking (sentry, etc.)
- Concat√®ne plusieurs emails avec des virgules

### **extract_emails_db.py**
**Exporte les emails de la base vers CSV**
- G√©n√®re un CSV avec tous les sites et leurs emails
- Utile pour backup ou analyse

**Usage:**
```bash
python3 extract_emails_db.py
```

---

## üï∑Ô∏è Scripts de Scraping

### **extract_emails.py**
**Scraper principal pour extraire les emails**
- Lit une liste de domaines
- Crawl chaque site pour trouver des emails
- Sauvegarde dans la base de donn√©es
- Filtre les emails invalides

**Usage:**
```bash
python3 extract_emails.py domains.txt
```

### **playwright_crawl.py**
**Scraper utilisant Playwright (navigateur headless)**
- Plus robuste que requests pour les sites JS
- Simule un vrai navigateur
- G√®re les cookies, redirections, etc.

### **selenium_crawl.py**
**Scraper utilisant Selenium**
- Alternative √† Playwright
- Pour sites n√©cessitant interaction JavaScript

### **crawl_backlinks.py**
**Crawl les backlinks d'un site**
- Trouve les sites qui pointent vers un domaine
- Utile pour d√©couvrir de nouveaux prospects

---

## üìä Scripts Google Sheets

### **upload_to_gsheet.py**
**Upload les donn√©es vers Google Sheets**
- Met √† jour la Feuille 1 avec les r√©sultats
- Synchronise base de donn√©es ‚Üí Google Sheets

### **upload_emails_to_gsheet.py**
**Upload uniquement les emails trouv√©s**
- Version sp√©cialis√©e pour les emails

### **upload_no_email_to_sheet.py**
**Upload les sites sans email vers une feuille**
- Utile pour identifier les sites √† retraiter

### **update_feuille1.py**
**Met √† jour la Feuille 1**
- Synchronisation bidirectionnelle

### **update_feuille2_batch.py**
**Met √† jour la Feuille 2 par batch**
- Pour √©viter les timeouts sur gros volumes

### **create_feuille3.py**
**Cr√©e la Feuille 3 avec les emails trouv√©s via SIRET**
- S√©pare les sources d'emails (scraping vs SIRET)

### **update_sheet_with_leaders_playwright.py**
**Met √† jour le sheet avec les dirigeants (Playwright)**
- Scrape les informations de dirigeants
- Version Playwright (plus fiable)

### **update_sheet_with_leaders.py**
**Version requests du script pr√©c√©dent**

### **update_feuille2_with_leaders_playwright.py**
**Sp√©cifique √† la Feuille 2**

### **update_feuille2_with_leaders.py**
**Version requests**

---

## üîç Scripts de Recherche SIRET/Dirigeants

### **find_company_leaders.py**
**Trouve les dirigeants d'une entreprise**
- Utilise API Pappers ou scraping societe.com
- Stocke dans le champ `leaders`

**Usage:**
```bash
python3 find_company_leaders.py
```

### **fetch_dirigeants_slow.py**
**Version "slow" avec rate limiting**
- √âvite de se faire bloquer
- Ajoute des d√©lais entre requ√™tes

### **fetch_emails_from_pappers.py**
**R√©cup√®re les emails via l'API Pappers**
- Utilise les SIRET pour trouver emails
- Cl√© API: `9c9507b8e254e643ae1040e87eb573fed6f1d6dfc6049c74`
- ‚ö†Ô∏è N√©cessite des cr√©dits API (100 gratuits)

**Modes:**
```bash
# Mode test (1 domaine)
python3 fetch_emails_from_pappers.py test

# Mode dry-run (simulation)
python3 fetch_emails_from_pappers.py --dry-run

# Mode production
python3 fetch_emails_from_pappers.py
```

### **check_pappers_potential.py**
**Analyse le potentiel de l'API Pappers**
- Compte combien de sites ont SIRET mais pas email
- Estime le co√ªt en cr√©dits API
- Projette le taux d'emails apr√®s utilisation

**Usage:**
```bash
python3 check_pappers_potential.py
```

---

## üîß Scripts Utilitaires

### **check_progress.py**
**Affiche la progression du scraping**
- Statistiques en temps r√©el
- Nombre de sites trait√©s, emails trouv√©s, etc.

**Usage:**
```bash
python3 check_progress.py
```

### **clean_emails.py**
**Nettoie les emails trouv√©s**
- Filtre les emails invalides
- Supprime les doublons
- Retire les emails de tracking (sentry, etc.)

### **clean_feuille2.py**
**Nettoie la Feuille 2 du Google Sheet**

### **clean_unwanted_domains.py**
**Supprime les domaines non d√©sir√©s**
- Filtre selon patterns (spam, parked domains, etc.)

### **extract_domains.py**
**Extrait les domaines d'une source**
- Parse HTML, CSV, ou autre pour extraire domaines

### **find_new_prospects.py**
**Trouve de nouveaux prospects**
- Algorithmes de d√©couverte de domaines similaires

### **format_for_gsheet.py**
**Formate les donn√©es pour Google Sheets**
- Pr√©pare les donn√©es au bon format

### **retry_failed_domains.py**
**Retente les domaines en erreur**
- Relance le scraping pour les sites failed

**Usage:**
```bash
python3 retry_failed_domains.py
```

---

## üß™ Scripts de Test

### **test_20min.py**
**Test de scraping sur 20minutes.fr**

### **test_bordas.py**
**Test de scraping sur bordas.fr**

### **test_pappers.py**
**Test de l'API Pappers**
- V√©rifie la connexion et les cr√©dits

**Usage:**
```bash
python3 test_pappers.py
```

### **test_playwright_siret.py**
**Test de recherche SIRET avec Playwright**

### **test_societe_playwright.py**
**Test de scraping societe.com avec Playwright**

### **scraper_bijouxenvogue.fr**
**Test sp√©cifique pour un site e-commerce**

---

## üìà Workflow Recommand√©

### 1Ô∏è‚É£ **Import Initial**
```bash
# Importer les emails depuis Google Sheets Feuille 1
python3 import_feuille1_emails.py

# V√©rifier les stats
python3 check_progress.py
```

### 2Ô∏è‚É£ **Scraping de Nouveaux Sites**
```bash
# Ajouter des domaines
echo "example.fr" >> domains_new.txt

# Lancer le scraping
python3 extract_emails.py domains_new.txt

# Ou via l'API
curl -X POST https://admin.perfect-cocon-seo.fr/api/scrape \
  -H "Content-Type: application/json" \
  -d '{"domains": ["example.fr"]}'
```

### 3Ô∏è‚É£ **Enrichissement via SIRET**
```bash
# V√©rifier le potentiel
python3 check_pappers_potential.py

# Lancer la recherche (si cr√©dits disponibles)
python3 fetch_emails_from_pappers.py
```

### 4Ô∏è‚É£ **Synchronisation Google Sheets**
```bash
# Upload vers Google Sheets
python3 upload_to_gsheet.py

# Ou import depuis Google Sheets
python3 import_feuille1_emails.py
```

### 5Ô∏è‚É£ **V√©rification & Monitoring**
```bash
# Statistiques via API
curl -s https://admin.perfect-cocon-seo.fr/api/stats | python3 -m json.tool

# Ou via interface web
# https://admin.perfect-cocon-seo.fr
```

---

## üîê Configuration Requise

### Fichiers de Configuration
- `credentials.json` - Credentials Google Sheets API
- `scrap_email.db` - Base de donn√©es SQLite
- `.env` - Variables d'environnement (optionnel)

### Services Syst√®me
- `scrap-email-interface.service` - Service systemd pour l'app Flask
- Nginx reverse proxy configur√© sur port 443 (HTTPS)
- Certificat SSL Let's Encrypt actif

### APIs Utilis√©es
- **Google Sheets API** - Synchronisation donn√©es
- **Pappers API** - Recherche d'emails via SIRET
  - Cl√©: `9c9507b8e254e643ae1040e87eb573fed6f1d6dfc6049c74`
  - Cr√©dits: 100 gratuits (√† activer)

---

## üìä √âtat Actuel de la Base

### Statistiques (au 18 octobre 2025)
- **Total sites**: 2850
- **Sites avec email**: 1182 (41.5%)
  - Scraping: 1182
  - SIRET: 0
- **Sites avec SIRET**: 820 (28.8%)
- **Sites avec dirigeants**: 74 (2.6%)
- **Sites complets**: 74 (2.6%)

### Potentiel d'Am√©lioration
- ~750 sites ont SIRET mais pas email
- Utilisation API Pappers ‚Üí estim√© +560 emails (75% succ√®s)
- Co√ªt estim√©: 15‚Ç¨ (750 √ó 0.02‚Ç¨)

---

## üö® Points d'Attention

### ‚ö†Ô∏è Ne PAS Ex√©cuter Plusieurs Fois
- `migrate_add_email_source.py` - Migration d√©j√† effectu√©e

### üîÑ Scripts de Maintenance R√©guli√®re
- `check_progress.py` - Monitoring
- `retry_failed_domains.py` - Relance erreurs
- `import_feuille1_emails.py` - Sync Google Sheets ‚Üí DB

### üõ°Ô∏è Rate Limiting
- Utiliser les versions `_slow` pour √©viter blocages
- Respecter les ToS des sites scrap√©s
- API Pappers: limites selon abonnement

---

## üìû Support & Documentation

### Logs
```bash
# Logs du service Flask
sudo journalctl -u scrap-email-interface.service -f

# Logs Nginx
sudo tail -f /var/log/nginx/error.log
```

### Commandes Utiles
```bash
# Red√©marrer le service
sudo systemctl restart scrap-email-interface.service

# V√©rifier l'√©tat
sudo systemctl status scrap-email-interface.service

# Tester Nginx
sudo nginx -t
```

### URLs Importantes
- **Interface Admin**: https://admin.perfect-cocon-seo.fr
- **API Stats**: https://admin.perfect-cocon-seo.fr/api/stats
- **Google Sheet**: https://docs.google.com/spreadsheets/d/19p41GglQIybuD1MynMIOgtmWjNHfOAU9foLEzJN-t6I

---

**Derni√®re mise √† jour**: 18 octobre 2025
