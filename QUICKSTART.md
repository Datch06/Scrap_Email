# Guide de D√©marrage Rapide - Interface Scrap Email

## üöÄ Installation en 3 minutes

### √âtape 1 : Installer les d√©pendances

```bash
cd /var/www/Scrap_Email
pip3 install sqlalchemy flask flask-cors
```

### √âtape 2 : Initialiser la base de donn√©es

```bash
python3 database.py
```

Vous verrez : `‚úì Base de donn√©es cr√©√©e avec succ√®s : scrap_email.db`

### √âtape 3 : Importer vos donn√©es existantes (optionnel)

Si vous avez d√©j√† des fichiers CSV/JSON avec des donn√©es :

```bash
python3 import_existing_data.py
```

### √âtape 4 : Lancer l'interface

```bash
python3 app.py
```

### √âtape 5 : Ouvrir l'interface

Ouvrez votre navigateur : **http://localhost:5000**

---

## üìä Utilisation de l'interface

### Dashboard (Page d'accueil)
- **Statistiques en temps r√©el** : Total sites, emails trouv√©s, SIRET, dirigeants
- **Graphiques interactifs** : R√©partition par statut, taux de compl√©tion
- **Actions rapides** : Acc√®s direct aux filtres et exports

### Page Sites
- **Tableau complet** de tous vos sites
- **Filtres** : Par statut, email, SIRET, dirigeants
- **Recherche** : Rechercher un domaine sp√©cifique
- **Actions** : Voir d√©tails, supprimer
- **Ajouter** : Ajouter manuellement de nouveaux sites

### Page Jobs
- **Historique** des t√¢ches de scraping
- **Progression** en temps r√©el
- **Statistiques** de succ√®s/erreurs

---

## üîß Int√©grer vos scripts existants

### Exemple 1 : Ajouter des domaines d√©couverts

```python
from db_helper import DBHelper

domains = ['site1.fr', 'site2.fr', 'site3.fr']

with DBHelper() as db:
    for domain in domains:
        db.add_site(domain, source_url='https://source.com')
```

### Exemple 2 : Mettre √† jour avec des emails

```python
from db_helper import DBHelper

results = {
    'site1.fr': 'contact@site1.fr',
    'site2.fr': 'NO EMAIL FOUND',
    'site3.fr': 'info@site3.fr; sales@site3.fr'
}

with DBHelper() as db:
    for domain, emails in results.items():
        db.update_email(domain, emails)
```

### Exemple 3 : Workflow complet

```python
from db_helper import DBHelper

with DBHelper() as db:
    # R√©cup√©rer les sites sans email
    sites = db.get_sites_without_email(limit=10)

    for site in sites:
        domain = site.domain

        # Votre code pour extraire l'email
        emails = extract_emails_from_domain(domain)

        # Mettre √† jour la base
        if emails:
            db.update_email(domain, '; '.join(emails))
        else:
            db.update_email(domain, 'NO EMAIL FOUND')
```

---

## üì§ Export des donn√©es

### Depuis l'interface web
1. Cliquer sur **"Exporter CSV"** dans le dashboard
2. Le fichier sera t√©l√©charg√© automatiquement

### Via API
```bash
curl http://localhost:5000/api/export/csv -o export.csv
```

### Vers Google Sheets
Utilisez vos scripts existants `upload_to_gsheet.py` en r√©cup√©rant les donn√©es depuis la base :

```python
from db_helper import DBHelper

with DBHelper() as db:
    # R√©cup√©rer tous les sites avec email + SIRET + dirigeants
    query = db.session.query(Site).filter(
        Site.emails.isnot(None),
        Site.siret.isnot(None),
        Site.leaders.isnot(None)
    ).all()

    # Pr√©parer pour Google Sheets
    data = [[site.domain, site.emails, site.siret, site.leaders]
            for site in query]
```

---

## üéØ Workflows recommand√©s

### Workflow 1 : D√©couverte de nouveaux sites
```bash
# 1. Crawl pour d√©couvrir des sites
python3 playwright_crawl.py --start https://www.ladepeche.fr/ --max-pages 100

# 2. Extraire les domaines
python3 extract_domains.py

# 3. Importer dans la base
python3 import_existing_data.py

# 4. V√©rifier dans l'interface
# Ouvrir http://localhost:5000/sites
```

### Workflow 2 : Extraire les emails
```bash
# 1. Lister les sites sans email via l'interface
# Filtre : has_email=false

# 2. Lancer l'extraction (modifi√© pour utiliser la DB)
python3 extract_emails_db.py --limit 50

# 3. Voir les r√©sultats dans l'interface
# Rafra√Æchir la page Sites
```

### Workflow 3 : Compl√©ter avec SIRET et dirigeants
```bash
# 1. Sites avec email mais sans SIRET
# Filtre : has_email=true, has_siret=false

# 2. Chercher SIRET
python3 find_siret_db.py --limit 20

# 3. Sites avec SIRET mais sans dirigeants
# Filtre : has_siret=true, has_leaders=false

# 4. Chercher dirigeants
python3 find_leaders_db.py --limit 10
```

---

## üîç API REST

### Obtenir des statistiques
```bash
curl http://localhost:5000/api/stats
```

### Lister les sites
```bash
# Tous les sites
curl http://localhost:5000/api/sites

# Avec filtres
curl "http://localhost:5000/api/sites?has_email=true&has_siret=false"
```

### Ajouter un site
```bash
curl -X POST http://localhost:5000/api/sites \
  -H "Content-Type: application/json" \
  -d '{"domain": "example.fr", "source_url": "https://source.com"}'
```

### Mettre √† jour un site
```bash
curl -X PUT http://localhost:5000/api/sites/1 \
  -H "Content-Type: application/json" \
  -d '{"emails": "contact@example.fr"}'
```

---

## üêõ D√©pannage

### Probl√®me : "ModuleNotFoundError: No module named 'sqlalchemy'"
**Solution** :
```bash
pip3 install sqlalchemy flask flask-cors
```

### Probl√®me : "database is locked"
**Solution** : Une seule instance de l'application peut acc√©der √† la base √† la fois. Fermez les autres processus.

### Probl√®me : L'interface ne charge pas
**Solution** :
1. V√©rifiez que le serveur est lanc√© : `python3 app.py`
2. V√©rifiez l'URL : http://localhost:5000
3. Regardez les logs dans le terminal

### Probl√®me : Pas de donn√©es affich√©es
**Solution** :
1. Importez vos donn√©es : `python3 import_existing_data.py`
2. Ou ajoutez manuellement via l'interface

---

## üìù Notes importantes

- **Sauvegarde** : La base de donn√©es est dans le fichier `scrap_email.db`. Sauvegardez-le r√©guli√®rement !
- **Performance** : SQLite est parfait pour jusqu'√† ~100 000 sites. Au-del√†, envisagez PostgreSQL
- **S√©curit√©** : Par d√©faut, l'interface est accessible uniquement en local. Pour un d√©ploiement distant, ajoutez une authentification
- **Auto-refresh** : Le dashboard se rafra√Æchit automatiquement toutes les 30 secondes

---

## üéâ Vous √™tes pr√™t !

L'interface est maintenant op√©rationnelle. Bon scraping ! üöÄ
