#!/usr/bin/env python3
"""
Scraper ASYNCHRONE ultra-rapide pour LinkAvista MarketLink
Utilise asyncio + aiohttp pour scraper 2000+ sites/minute

Performance attendue: 4-5x plus rapide que le scraper synchrone
"""

import asyncio
import aiohttp
import time
import re
from bs4 import BeautifulSoup
from database import get_session, Site, SiteStatus
from datetime import datetime
from typing import List, Set, Optional
import sys
from email_finder_async import AsyncEmailFinder

class AsyncLinkAvistaScraper:
    def __init__(self, email: str, password: str, max_concurrent: int = 50):
        """
        Initialiser le scraper asynchrone

        Args:
            email: Email LinkAvista
            password: Mot de passe LinkAvista
            max_concurrent: Nombre de requ√™tes simultan√©es (d√©faut: 50)
        """
        self.email = email
        self.password = password
        self.max_concurrent = max_concurrent
        self.base_url = "https://linkavista.com"
        self.session = None
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

    async def login(self) -> bool:
        """Se connecter √† LinkAvista de mani√®re asynchrone"""
        print(f"üîê Connexion √† LinkAvista...")

        try:
            # Cr√©er une session aiohttp avec cookie jar
            connector = aiohttp.TCPConnector(limit=self.max_concurrent, ssl=False)
            timeout = aiohttp.ClientTimeout(total=30)
            self.session = aiohttp.ClientSession(
                connector=connector,
                timeout=timeout,
                headers=self.headers
            )

            # Obtenir le CSRF token
            async with self.session.get(f"{self.base_url}/login") as response:
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                csrf_input = soup.find('input', {'name': '_token'})
                csrf_token = csrf_input.get('value') if csrf_input else None

            # Se connecter
            login_data = {
                'email': self.email,
                'password': self.password,
            }
            if csrf_token:
                login_data['_token'] = csrf_token

            async with self.session.post(
                f"{self.base_url}/login",
                data=login_data,
                allow_redirects=True
            ) as response:
                html = await response.text()

                if 'logout' in html.lower() and response.status == 200:
                    print("‚úÖ Connexion r√©ussie!")
                    return True
                else:
                    print(f"‚ùå √âchec de connexion")
                    return False

        except Exception as e:
            print(f"‚ùå Erreur de connexion: {e}")
            return False

    async def extract_urls_from_page(self, page_num: int, filter_type: str = "normal") -> List[str]:
        """Extraire toutes les URLs d'une page de mani√®re asynchrone"""
        try:
            # Construire l'URL selon le filtre
            if filter_type == "normal":
                url = f"{self.base_url}/marketlink" if page_num == 1 else f"{self.base_url}/marketlink/page/{page_num}"
            elif filter_type == "sensitive":
                if page_num == 1:
                    url = f"{self.base_url}/marketlink?show_sensitive_categories=on&sensitive_category_id=1"
                else:
                    url = f"{self.base_url}/marketlink/page/{page_num}?show_sensitive_categories=on&sensitive_category_id=1"
            elif filter_type == "gnews":
                if page_num == 1:
                    url = f"{self.base_url}/marketlink?is_gnews=on"
                else:
                    url = f"{self.base_url}/marketlink/page/{page_num}?is_gnews=on"
            else:
                return []

            async with self.session.get(url, timeout=aiohttp.ClientTimeout(total=15)) as response:
                if response.status != 200:
                    return []

                html = await response.text()

                # Extraire les URLs
                urls = re.findall(r'"url":"([^"]+)"', html)
                urls = list(set(urls))  # D√©dupliquer

                return urls

        except asyncio.TimeoutError:
            return []
        except Exception as e:
            return []

    async def search_email_on_site(self, domain: str) -> Optional[str]:
        """Chercher un email sur un site de mani√®re asynchrone avec le finder avanc√©"""
        # Utiliser le AsyncEmailFinder pour une recherche plus compl√®te
        finder = AsyncEmailFinder(self.session)
        return await finder.search_emails_on_domain(domain, max_pages=8)

    async def process_domain(self, domain: str, db_session, stats: dict) -> None:
        """Traiter un domaine: chercher email et ajouter en base"""

        # Exclure les domaines .gouv.fr
        if domain.endswith('.gouv.fr'):
            stats['skipped'] += 1
            return

        # V√©rifier si d√©j√† en base
        existing = db_session.query(Site).filter_by(domain=domain).first()
        if existing:
            stats['skipped'] += 1
            return

        # Chercher l'email
        email = await self.search_email_on_site(domain)

        if email:
            stats['emails_found'] += 1

        # Cr√©er l'entr√©e en base
        site = Site(
            domain=domain,
            source_url="https://linkavista.com/marketlink",
            emails=email if email else "NO EMAIL FOUND",
            email_checked=True,
            email_found_at=datetime.utcnow() if email else None,
            email_source="linkavista_async_scraping",
            status=SiteStatus.EMAIL_FOUND if email else SiteStatus.EMAIL_NOT_FOUND,
            created_at=datetime.utcnow()
        )

        db_session.add(site)

        try:
            db_session.commit()
            stats['added'] += 1
        except Exception as e:
            # Rollback en cas d'erreur (doublon concurrent)
            db_session.rollback()
            stats['skipped'] += 1

    async def process_domains_batch(self, domains: List[str], db_session, stats: dict, semaphore: asyncio.Semaphore) -> None:
        """Traiter un lot de domaines avec limitation de concurrence"""
        tasks = []

        for domain in domains:
            async with semaphore:
                task = self.process_domain(domain, db_session, stats)
                tasks.append(task)

        await asyncio.gather(*tasks, return_exceptions=True)

    async def extract_all_domains(self, max_pages: int = None) -> Set[str]:
        """Extraire tous les domaines avec tous les filtres de mani√®re asynchrone - SANS LIMITE"""
        print("\nüì• PHASE 1: Extraction ASYNCHRONE de tous les domaines (SANS LIMITE)")
        print("="*80)

        all_domains = set()

        # Configuration des filtres - on scrape jusqu'√† ce qu'il n'y ait plus de r√©sultats
        filters = [
            ("Normal", "normal"),
            ("Sensitive", "sensitive"),
            ("Google News", "gnews"),
        ]

        for filter_name, filter_type in filters:
            print(f"\nüîç Filtre: {filter_name}")
            print("-"*80)

            initial_count = len(all_domains)
            page = 1
            consecutive_empty = 0

            # Continuer jusqu'√† avoir 3 pages vides cons√©cutives
            while consecutive_empty < 3:
                urls = await self.extract_urls_from_page(page, filter_type)

                if isinstance(urls, Exception) or not urls:
                    consecutive_empty += 1
                    print(f"üìÑ Page {page:4} ‚Üí Vide ({consecutive_empty}/3)")
                    page += 1
                    continue

                new_urls = [u for u in urls if u not in all_domains]
                all_domains.update(urls)

                if len(new_urls) == 0:
                    consecutive_empty += 1
                    print(f"üìÑ Page {page:4} ‚Üí {len(urls):3} sites (0 nouveaux) | Total: {len(all_domains):,} ({consecutive_empty}/3)")
                else:
                    consecutive_empty = 0
                    print(f"üìÑ Page {page:4} ‚Üí {len(urls):3} sites (+{len(new_urls):3} nouveaux) | Total: {len(all_domains):,}")

                page += 1

            filter_added = len(all_domains) - initial_count
            print(f"‚úÖ {filter_name}: +{filter_added:,} domaines suppl√©mentaires (arr√™t √† la page {page-1})")

        print(f"\nüéØ TOTAL FINAL: {len(all_domains):,} domaines uniques extraits")
        return all_domains

    async def scrape_async(self, batch_size: int = 50):
        """
        Scraper asynchrone complet - SANS LIMITE de pages

        Args:
            max_pages: Nombre maximum de pages √† scraper par filtre
            batch_size: Taille des lots pour le traitement parall√®le
        """
        start_time = time.time()

        print("\n" + "="*80)
        print("üöÄ SCRAPING LINKAVISTA ASYNCHRONE - ULTRA RAPIDE")
        print("="*80)
        print(f"   Concurrence: {self.max_concurrent} requ√™tes simultan√©es")
        print(f"   Batch size: {batch_size} domaines par lot")
        print("="*80)

        # Se connecter
        if not await self.login():
            return

        try:
            # PHASE 1: Extraire tous les domaines (sans limite)
            all_domains = await self.extract_all_domains()

            extraction_time = time.time() - start_time
            print(f"\n‚è±Ô∏è  Temps d'extraction: {extraction_time:.1f}s ({len(all_domains)/extraction_time:.1f} domaines/sec)")

            # PHASE 2: Recherche d'emails et ajout en base
            print("\nüìß PHASE 2: Recherche d'emails ASYNCHRONE et ajout en base")
            print("="*80)

            db_session = get_session()
            stats = {
                'added': 0,
                'skipped': 0,
                'emails_found': 0,
            }

            # Cr√©er un s√©maphore pour limiter la concurrence
            semaphore = asyncio.Semaphore(self.max_concurrent)

            # Traiter les domaines par lots
            domains_list = sorted(all_domains)
            total_domains = len(domains_list)

            for i in range(0, total_domains, batch_size):
                batch = domains_list[i:i+batch_size]
                batch_start = time.time()

                print(f"\nüîÑ Traitement du lot {i//batch_size + 1}/{(total_domains + batch_size - 1)//batch_size} ({len(batch)} domaines)...")

                # Traiter le lot en parall√®le
                tasks = []
                for domain in batch:
                    async with semaphore:
                        task = self.process_domain(domain, db_session, stats)
                        tasks.append(task)

                await asyncio.gather(*tasks, return_exceptions=True)

                batch_time = time.time() - batch_start
                speed = len(batch) / batch_time if batch_time > 0 else 0

                print(f"‚úÖ Lot trait√© en {batch_time:.1f}s ({speed:.1f} sites/sec)")
                print(f"   Ajout√©s: {stats['added']:,} | Ignor√©s: {stats['skipped']:,} | Emails: {stats['emails_found']:,}")

            db_session.close()

            # R√©sum√© final
            total_time = time.time() - start_time

            print("\n" + "="*80)
            print("‚úÖ SCRAPING ASYNCHRONE TERMIN√â!")
            print("="*80)
            print(f"   Temps total: {total_time:.1f}s ({total_time/60:.1f} minutes)")
            print(f"   Domaines extraits: {len(all_domains):,}")
            print(f"   Sites ajout√©s: {stats['added']:,}")
            print(f"   Sites ignor√©s: {stats['skipped']:,}")
            print(f"   Emails trouv√©s: {stats['emails_found']:,}")
            if stats['added'] > 0:
                print(f"   Taux de d√©couverte: {(stats['emails_found']/stats['added']*100):.1f}%")
            print(f"   Vitesse moyenne: {len(all_domains)/total_time:.1f} domaines/sec")
            print(f"   Gain de performance: ~4-5x plus rapide que le scraper synchrone")
            print("="*80)
            print("\nüéØ Consultez l'admin: https://admin.perfect-cocon-seo.fr")

        finally:
            # Fermer la session aiohttp
            if self.session:
                await self.session.close()


async def main():
    """Point d'entr√©e principal"""
    EMAIL = "datchdigital@gmail.com"
    PASSWORD = "B-BJoqV7"
    MAX_CONCURRENT = 50  # Nombre de requ√™tes simultan√©es
    BATCH_SIZE = 100     # Taille des lots pour le traitement

    scraper = AsyncLinkAvistaScraper(
        email=EMAIL,
        password=PASSWORD,
        max_concurrent=MAX_CONCURRENT
    )

    await scraper.scrape_async(
        batch_size=BATCH_SIZE
    )


if __name__ == "__main__":
    # Configurer la politique d'event loop pour Windows (si n√©cessaire)
    if sys.platform.startswith('win'):
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

    # Lancer le scraper asynchrone
    asyncio.run(main())
