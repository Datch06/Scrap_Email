# ğŸ‰ Nouvelle Interface de Gestion - Scrap Email

## RÃ©sumÃ© de l'amÃ©lioration

Vous disposez maintenant d'une **interface web complÃ¨te** pour gÃ©rer votre systÃ¨me de scraping avec :

- âœ… **Base de donnÃ©es SQLite** pour stocker tous les sites et leur Ã©tat
- âœ… **Dashboard interactif** avec statistiques en temps rÃ©el
- âœ… **Suivi dÃ©taillÃ©** de chaque Ã©tape (dÃ©couverte â†’ email â†’ SIRET â†’ dirigeants)
- âœ… **API REST** pour intÃ©gration avec vos scripts
- âœ… **Filtres avancÃ©s** et recherche
- âœ… **Export CSV** et Google Sheets

---

## ğŸ“ Nouveaux fichiers crÃ©Ã©s

### Backend
- **`database.py`** - ModÃ¨les de base de donnÃ©es (Sites, Jobs)
- **`app.py`** - Application Flask avec API REST
- **`db_helper.py`** - Utilitaire pour faciliter l'intÃ©gration

### Frontend
- **`templates/base.html`** - Template de base
- **`templates/index.html`** - Dashboard avec graphiques
- **`templates/sites.html`** - Gestion des sites
- **`templates/jobs.html`** - Suivi des jobs
- **`static/css/style.css`** - Styles personnalisÃ©s

### Utilitaires
- **`import_existing_data.py`** - Import de vos donnÃ©es existantes
- **`extract_emails_db.py`** - Exemple de script avec DB
- **`requirements_interface.txt`** - DÃ©pendances Python

### Documentation
- **`README_INTERFACE.md`** - Documentation complÃ¨te
- **`QUICKSTART.md`** - Guide de dÃ©marrage rapide
- **`NOUVELLE_INTERFACE.md`** - Ce fichier

---

## ğŸš€ Mise en route (3 commandes)

```bash
# 1. Installer les dÃ©pendances
pip3 install sqlalchemy flask flask-cors

# 2. CrÃ©er la base de donnÃ©es
python3 database.py

# 3. Lancer l'interface
python3 app.py
```

Ouvrez ensuite : **http://localhost:5000**

---

## ğŸ“Š FonctionnalitÃ©s principales

### 1. Dashboard (/)
![Dashboard]
- **Cartes de statistiques** : Total sites, emails, SIRET, dirigeants
- **Graphiques** :
  - RÃ©partition par statut (camembert)
  - Taux de complÃ©tion (barres)
- **Actions rapides** : Liens vers filtres prÃ©dÃ©finis
- **Auto-refresh** : Mise Ã  jour automatique toutes les 30s

### 2. Gestion des Sites (/sites)
- **Tableau paginÃ©** avec tous vos sites (50 par page)
- **Filtres** :
  - Par statut (dÃ©couvert, email trouvÃ©, SIRET trouvÃ©, etc.)
  - Par prÃ©sence d'email (oui/non)
  - Par prÃ©sence de SIRET (oui/non)
  - Par prÃ©sence de dirigeants (oui/non)
  - Recherche par domaine
- **Actions** :
  - Voir dÃ©tails complets d'un site
  - Supprimer un site
  - Ajouter manuellement un site
- **Export** : TÃ©lÃ©charger en CSV

### 3. Suivi des Jobs (/jobs)
- **Historique** de toutes les tÃ¢ches de scraping
- **Progression** en temps rÃ©el avec barre de progression
- **Statistiques** : SuccÃ¨s/Erreurs par job
- **DurÃ©e** d'exÃ©cution

---

## ğŸ”„ Statuts des sites

La base de donnÃ©es suit automatiquement l'Ã©tat de chaque site :

1. **`discovered`** - Site dÃ©couvert, non traitÃ©
2. **`email_found`** - Email trouvÃ©
3. **`email_not_found`** - Email non trouvÃ©
4. **`siret_found`** - SIRET/SIREN trouvÃ©
5. **`siret_not_found`** - SIRET non trouvÃ©
6. **`leaders_found`** - Dirigeants trouvÃ©s
7. **`completed`** - Toutes les donnÃ©es rÃ©cupÃ©rÃ©es
8. **`error`** - Erreur lors du traitement

---

## ğŸ”Œ IntÃ©gration avec vos scripts

### MÃ©thode 1 : Utiliser le DBHelper

```python
from db_helper import DBHelper

with DBHelper() as db:
    # Ajouter un site
    site = db.add_site('example.fr', 'https://source.com')

    # Mettre Ã  jour avec email
    db.update_email('example.fr', 'contact@example.fr')

    # Mettre Ã  jour avec SIRET
    db.update_siret('example.fr', '12345678901234', 'SIRET')

    # Mettre Ã  jour avec dirigeants
    db.update_leaders('example.fr', ['Jean Dupont', 'Marie Martin'])

    # RÃ©cupÃ©rer les sites Ã  traiter
    sites_sans_email = db.get_sites_without_email(limit=100)
    sites_sans_siret = db.get_sites_without_siret(limit=100)
    sites_sans_leaders = db.get_sites_without_leaders(limit=100)
```

### MÃ©thode 2 : Utiliser l'API REST

```bash
# Ajouter un site
curl -X POST http://localhost:5000/api/sites \
  -H "Content-Type: application/json" \
  -d '{"domain": "example.fr"}'

# Mettre Ã  jour un site
curl -X PUT http://localhost:5000/api/sites/1 \
  -H "Content-Type: application/json" \
  -d '{"emails": "contact@example.fr"}'

# RÃ©cupÃ©rer les statistiques
curl http://localhost:5000/api/stats
```

---

## ğŸ“– Exemples de workflows

### Workflow 1 : Import de donnÃ©es existantes

```bash
# Importer tous vos fichiers CSV/JSON/TXT existants
python3 import_existing_data.py

# VÃ©rifier dans l'interface
# â†’ http://localhost:5000
```

### Workflow 2 : Extraction d'emails

```bash
# Lancer l'extraction pour 50 sites
python3 extract_emails_db.py --limit 50

# Voir les rÃ©sultats en temps rÃ©el dans l'interface
# â†’ http://localhost:5000/sites?has_email=true
```

### Workflow 3 : Recherche SIRET et dirigeants

```python
# CrÃ©er un script similaire pour SIRET
from db_helper import DBHelper
from find_company_leaders import find_siret_siren, fetch_company_leaders

with DBHelper() as db:
    # RÃ©cupÃ©rer sites avec email mais sans SIRET
    sites = db.get_sites_without_siret(limit=20)

    for site in sites:
        siret, siret_type = find_siret_siren(site.domain, opener)
        if siret:
            db.update_siret(site.domain, siret, siret_type)

            # Chercher dirigeants
            leaders = fetch_company_leaders(siret, siret_type, opener)
            if leaders:
                db.update_leaders(site.domain, leaders)
```

---

## ğŸ“ˆ Avantages de la nouvelle architecture

### Avant (fichiers CSV/JSON)
- âŒ Difficile de suivre l'Ã©tat des sites
- âŒ DonnÃ©es dispersÃ©es dans plusieurs fichiers
- âŒ Pas de vue d'ensemble
- âŒ Risque de traiter deux fois les mÃªmes sites
- âŒ Difficile de reprendre aprÃ¨s une erreur

### Maintenant (Base de donnÃ©es + Interface)
- âœ… **Ã‰tat centralisÃ©** : Tout dans une seule base
- âœ… **Suivi en temps rÃ©el** : Voir la progression dans l'interface
- âœ… **Ã‰viter les doublons** : La base vÃ©rifie automatiquement
- âœ… **Reprise sur erreur** : Voir exactement quels sites ont Ã©chouÃ©
- âœ… **Filtres puissants** : Trouver rapidement ce que vous cherchez
- âœ… **Statistiques** : Graphiques et mÃ©triques automatiques
- âœ… **Export facile** : CSV en un clic

---

## ğŸ¯ Cas d'usage typiques

### 1. "Je veux voir tous les sites avec email mais sans SIRET"
â†’ http://localhost:5000/sites?has_email=true&has_siret=false

### 2. "Combien de sites complets j'ai ?"
â†’ Dashboard â†’ Carte "Sites Complets"

### 3. "Quels sites ont eu des erreurs ?"
â†’ http://localhost:5000/sites?status=error

### 4. "Je veux exporter tous les sites avec dirigeants"
â†’ Filtrer â†’ has_leaders=true â†’ Exporter CSV

### 5. "Reprendre l'extraction d'emails aprÃ¨s une interruption"
```bash
python3 extract_emails_db.py --limit 100
# La base sait automatiquement quels sites n'ont pas encore Ã©tÃ© traitÃ©s
```

---

## ğŸ” SÃ©curitÃ© et bonnes pratiques

1. **Sauvegarde** : Sauvegardez rÃ©guliÃ¨rement `scrap_email.db`
   ```bash
   cp scrap_email.db scrap_email_backup_$(date +%Y%m%d).db
   ```

2. **AccÃ¨s local uniquement** : Par dÃ©faut, l'interface n'est accessible que depuis localhost

3. **Rate limiting** : Continuez Ã  respecter les dÃ©lais entre requÃªtes dans vos scripts

4. **Logs** : Les erreurs sont enregistrÃ©es dans la base (colonne `last_error`)

---

## ğŸ“Š Structure de la base de donnÃ©es

### Table `sites`
```sql
- id (PRIMARY KEY)
- domain (UNIQUE)
- status (discovered, email_found, etc.)
- emails
- siret / siren / siret_type
- leaders
- created_at / updated_at
- last_error
- retry_count
```

### Table `scraping_jobs`
```sql
- id (PRIMARY KEY)
- job_type (crawl, email, siret, leaders)
- status (pending, running, completed, failed)
- total_sites / processed_sites
- success_count / error_count
- start_time / end_time
```

---

## ğŸš€ Prochaines Ã©tapes recommandÃ©es

1. **Importer vos donnÃ©es** : `python3 import_existing_data.py`
2. **Tester l'interface** : `python3 app.py`
3. **Adapter vos scripts** : Utiliser `DBHelper` dans vos scripts existants
4. **Automatiser** : CrÃ©er des scripts cron pour lancer automatiquement les extractions

---

## ğŸ“ Support

Pour toute question :
1. Consultez `README_INTERFACE.md` pour la documentation complÃ¨te
2. Consultez `QUICKSTART.md` pour le guide rapide
3. Testez avec `db_helper.py` pour vÃ©rifier l'installation

---

## ğŸŠ FÃ©licitations !

Vous avez maintenant une solution professionnelle pour gÃ©rer votre scraping de donnÃ©es ! ğŸš€

L'interface vous permet de :
- âœ… Suivre l'Ã©tat de chaque site en temps rÃ©el
- âœ… Visualiser vos statistiques avec des graphiques
- âœ… Filtrer et rechercher facilement
- âœ… Exporter vos donnÃ©es
- âœ… Ã‰viter les doublons automatiquement
- âœ… Reprendre aprÃ¨s une erreur

**Bon scraping ! ğŸ¯**
