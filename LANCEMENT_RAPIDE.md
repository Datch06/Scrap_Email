# ğŸš€ LANCEMENT RAPIDE - Scraping Temps RÃ©el

## âš¡ EN 3 COMMANDES

```bash
# 1. Aller dans le dossier
cd /var/www/Scrap_Email

# 2. Lancer le scraping EN TEMPS RÃ‰EL (24/7)
nohup python3 scrape_realtime_complete.py > scraping_realtime.log 2>&1 &

# 3. Suivre la progression
tail -f scraping_realtime.log
```

**C'EST TOUT!** ğŸ‰

---

## ğŸ“Š Voir les RÃ©sultats EN DIRECT

**Interface Admin:** https://admin.perfect-cocon-seo.fr

RafraÃ®chir la page toutes les 30 secondes pour voir les nouveaux sites apparaÃ®tre!

---

## ğŸ¯ Ce qui se Passe Automatiquement

Le script va **EN CONTINU**:

1. âœ… Crawler **75,354 sites vendeurs** de backlinks
2. âœ… Extraire tous les domaines .fr acheteurs
3. âœ… Chercher leur **EMAIL** (5 pages/site)
4. âœ… Chercher leur **SIRET/SIREN** (7 pages lÃ©gales)
5. âœ… **UPLOADER IMMÃ‰DIATEMENT** dans l'admin
6. âœ… Recommencer indÃ©finiment jusqu'Ã  Ã©puisement

---

## ğŸ“ˆ RÃ©sultats Attendus

### Dans 1 Heure
- ~1,500 nouveaux sites
- ~600 emails
- ~420 SIRET

### Dans 1 Jour
- ~36,000 nouveaux sites
- ~14,400 emails
- ~10,000 SIRET

### Dans 1 Semaine
- ~250,000 nouveaux sites
- ~100,000 emails
- ~70,000 SIRET

### Dans 1 Mois
- ~1,000,000 nouveaux sites
- ~400,000 emails
- ~280,000 SIRET

### FINAL (3-4 mois)
- **~26,000,000 sites**
- **~10,400,000 emails** âœ‰ï¸
- **~7,300,000 SIRET** ğŸ¢

---

## ğŸ›‘ ArrÃªter

```bash
# Trouver le processus
ps aux | grep scrape_realtime

# Tuer (remplacer PID par le numÃ©ro)
kill -SIGINT PID

# Ou si vous avez sauvegardÃ© le PID
kill -SIGINT $(cat scraping_realtime.pid)
```

---

## ğŸ“Š VÃ©rifier que Ã‡a Tourne

```bash
# MÃ©thode 1: Processus
ps aux | grep scrape_realtime

# MÃ©thode 2: Logs rÃ©cents
tail -20 scraping_realtime.log

# MÃ©thode 3: Stats API
curl -s https://admin.perfect-cocon-seo.fr/api/stats | python3 -m json.tool

# MÃ©thode 4: Compter les nouveaux sites
sqlite3 scrap_email.db "SELECT COUNT(*) FROM sites WHERE created_at > datetime('now', '-1 hour');"
```

---

## âš™ï¸ Configuration (Optionnel)

**Fichier:** `scrape_realtime_complete.py`

### Vitesse
```python
PAUSE_BETWEEN_SITES = 0.1   # Plus petit = plus rapide (risque blocage)
PAUSE_BETWEEN_PAGES = 0.05
```

### Profondeur
```python
MAX_PAGES_PER_SELLER_SITE = 500  # Plus = plus de domaines trouvÃ©s
MAX_DEPTH = 5
```

---

## ğŸ”¥ Stats en Temps RÃ©el

```bash
# Dashboard auto-refresh (toutes les 5 secondes)
watch -n 5 'curl -s https://admin.perfect-cocon-seo.fr/api/stats | python3 -c "
import sys, json
d = json.load(sys.stdin)
print(f\"\"\"
Sites: {d[\"total_sites\"]}
Emails: {d[\"sites_with_email\"]} ({d[\"email_rate\"]}%)
SIRET: {d[\"sites_with_siret\"]} ({d[\"siret_rate\"]}%)
\"\"\")"'
```

---

## ğŸ“ Fichiers Importants

- **Script**: `scrape_realtime_complete.py`
- **Logs**: `scraping_realtime.log`
- **Base**: `scrap_email.db`
- **Progression**: `explored_seller_sites.txt`
- **Sites vendeurs**: `site_urls.txt` (75,354 sites)

---

## âœ… Checklist de VÃ©rification

- [ ] Script lancÃ© en arriÃ¨re-plan
- [ ] Logs qui dÃ©filent (tail -f)
- [ ] Nouveaux sites dans l'admin
- [ ] Stats qui augmentent
- [ ] Espace disque suffisant (df -h)

---

## ğŸ¯ Commande Ultime (Tout-en-Un)

```bash
cd /var/www/Scrap_Email && \
nohup python3 scrape_realtime_complete.py > scraping_realtime.log 2>&1 & \
echo $! > scraping_realtime.pid && \
echo "âœ… Scraping lancÃ©! PID: $(cat scraping_realtime.pid)" && \
echo "ğŸ“Š Admin: https://admin.perfect-cocon-seo.fr" && \
echo "ğŸ“ Logs: tail -f scraping_realtime.log" && \
sleep 2 && \
tail -f scraping_realtime.log
```

**Copy-paste cette commande et c'est parti!** ğŸš€

---

## ğŸ“ Aide Rapide

### ProblÃ¨me: Pas de nouveaux sites

```bash
# VÃ©rifier que le script tourne
ps aux | grep scrape_realtime

# VÃ©rifier les erreurs
tail -50 scraping_realtime.log | grep -i error
```

### ProblÃ¨me: Trop lent

```bash
# RÃ©duire les pauses (dans scrape_realtime_complete.py)
PAUSE_BETWEEN_SITES = 0.05
PAUSE_BETWEEN_PAGES = 0.02
```

### ProblÃ¨me: Espace disque plein

```bash
# VÃ©rifier l'espace
df -h /var/www

# Nettoyer les vieux logs
rm scraping_realtime_*.log

# Compresser la base si nÃ©cessaire
sqlite3 scrap_email.db "VACUUM;"
```

---

**Vous Ãªtes prÃªt! Lancez et regardez les millions de sites arriver!** ğŸ‰
