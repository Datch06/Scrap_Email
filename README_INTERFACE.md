# Interface de Gestion Scrap Email

Interface web moderne pour g√©rer et superviser vos op√©rations de scraping.

## Installation

### 1. Installer les d√©pendances

```bash
cd /var/www/Scrap_Email
pip install -r requirements_interface.txt
```

### 2. Initialiser la base de donn√©es

```bash
python3 database.py
```

Cela va cr√©er le fichier `scrap_email.db` avec toutes les tables n√©cessaires.

### 3. Importer vos donn√©es existantes (optionnel)

Si vous avez d√©j√† des donn√©es dans vos fichiers CSV ou JSON, vous pouvez les importer :

```bash
python3 import_existing_data.py
```

## Lancement de l'interface

```bash
python3 app.py
```

L'interface sera accessible sur : **http://localhost:5000**

## Fonctionnalit√©s

### üìä Dashboard
- Vue d'ensemble avec statistiques en temps r√©el
- Graphiques interactifs
- Taux de compl√©tion
- Activit√© r√©cente

### üåê Gestion des Sites
- Liste compl√®te de tous les sites
- Filtres avanc√©s (statut, email, SIRET, dirigeants)
- Recherche par domaine
- Ajout/suppression de sites
- Vue d√©taill√©e de chaque site

### ‚öôÔ∏è Gestion des Jobs
- Historique des t√¢ches de scraping
- Suivi de la progression en temps r√©el
- Statistiques de succ√®s/erreurs

### üì• Export
- Export CSV de toutes les donn√©es
- Donn√©es filtrables avant export

## Structure de la base de donn√©es

### Table `sites`
Stocke tous les sites d√©couverts avec :
- Domaine
- Statut (d√©couvert, email trouv√©, SIRET trouv√©, etc.)
- Emails
- SIRET/SIREN
- Dirigeants
- M√©tadonn√©es (dates, erreurs, retry count)

### Table `scraping_jobs`
Historique des jobs de scraping :
- Type de job
- Statut
- Progression
- R√©sultats (succ√®s/erreurs)

## Int√©gration avec vos scripts existants

### Utiliser le DBHelper

Le module `db_helper.py` facilite l'int√©gration :

```python
from db_helper import DBHelper

with DBHelper() as db:
    # Ajouter un site
    site = db.add_site('example.fr', source_url='https://...')

    # Mettre √† jour avec email
    db.update_email('example.fr', 'contact@example.fr')

    # Mettre √† jour avec SIRET
    db.update_siret('example.fr', '12345678901234', 'SIRET')

    # Mettre √† jour avec dirigeants
    db.update_leaders('example.fr', ['Jean Dupont', 'Marie Martin'])

    # R√©cup√©rer les sites sans email
    sites = db.get_sites_without_email(limit=100)
```

### Modifier vos scripts existants

Exemple pour `extract_emails.py` :

```python
from db_helper import DBHelper

# Au d√©but du script
with DBHelper() as db:
    # R√©cup√©rer les sites √† traiter
    sites = db.get_sites_without_email(limit=100)

    for site in sites:
        domain = site.domain

        # ... votre code existant pour extraire les emails ...

        # Mettre √† jour la base de donn√©es
        if emails_found:
            db.update_email(domain, '; '.join(emails_found))
        else:
            db.update_email(domain, 'NO EMAIL FOUND')
```

## API REST

L'interface expose une API REST compl√®te :

### Sites
- `GET /api/sites` - Liste des sites (avec pagination et filtres)
- `GET /api/sites/<id>` - D√©tails d'un site
- `POST /api/sites` - Cr√©er un site
- `PUT /api/sites/<id>` - Mettre √† jour un site
- `DELETE /api/sites/<id>` - Supprimer un site

### Statistiques
- `GET /api/stats` - Statistiques globales

### Jobs
- `GET /api/jobs` - Liste des jobs
- `POST /api/jobs` - Cr√©er un job
- `PUT /api/jobs/<id>` - Mettre √† jour un job

### Export
- `GET /api/export/csv` - Exporter en CSV

## Exemples de filtres

### Rechercher des sites
```
GET /api/sites?search=boutique&status=discovered
```

### Sites avec email mais sans SIRET
```
GET /api/sites?has_email=true&has_siret=false
```

### Sites complets (email + SIRET + dirigeants)
```
GET /api/sites?has_email=true&has_siret=true&has_leaders=true
```

## Workflow recommand√©

1. **D√©couverte** : Utiliser `playwright_crawl.py` pour d√©couvrir de nouveaux sites
2. **Import** : Ajouter les sites dans la base de donn√©es
3. **Emails** : Lancer l'extraction d'emails sur les sites sans email
4. **SIRET** : Chercher les SIRET pour les sites avec email
5. **Dirigeants** : Extraire les dirigeants pour les sites avec SIRET
6. **Export** : Exporter les donn√©es compl√®tes vers Google Sheets ou CSV

## S√©curit√©

- L'interface est accessible en local uniquement par d√©faut
- Pour un d√©ploiement en production, configurez :
  - Authentification
  - HTTPS
  - Firewall
  - Variables d'environnement pour les credentials

## Support

Pour toute question ou probl√®me :
1. V√©rifiez que la base de donn√©es est initialis√©e
2. V√©rifiez que toutes les d√©pendances sont install√©es
3. Consultez les logs dans la console

## Prochaines am√©liorations possibles

- [ ] Authentification utilisateur
- [ ] Planification automatique des jobs (cron)
- [ ] Notifications par email
- [ ] Webhooks pour int√©grations externes
- [ ] Dashboard multi-utilisateurs
- [ ] API GraphQL
